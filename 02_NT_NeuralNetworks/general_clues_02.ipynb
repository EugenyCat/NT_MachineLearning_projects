{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eeb25ec",
   "metadata": {},
   "source": [
    "<b>1. Datasets</b>\n",
    "\n",
    "<b>2. tensorflow keras things</b>\n",
    "\n",
    "<b>3. Build a binary classification model</b>\n",
    "\n",
    "<b>4. Нелийные НС</b>\n",
    "\n",
    "<b>5. Сверточные НС</b>\n",
    "\n",
    "<b>6. Рекурентные НС</b>\n",
    "\n",
    "<b>7. Evaluator - позволяет автоматизированно компилировать и получать скоры разных моделей</b>\n",
    "\n",
    "<b>8. Models - класс для построения моделей </b>\n",
    "\n",
    "<b>9.  Autoencoders (Автоэнкодер)</b>\n",
    "\n",
    "<b>10. Variational autoencoder (Вариационный автоэнкодер)</b>\n",
    "\n",
    "<b>11. Инструкция по Tensorboard</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905abba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd # Для работы с данными\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  # Библиотека для визуализации результатов\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfc9c7",
   "metadata": {},
   "source": [
    "# 1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37298d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19cb6e15850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "plt.imshow(x_test[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de75d8-5523-4a5a-bb65-6910a5a71f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CIFAR10\n",
    "# 60000 RGB pictires with size 32x32x3\n",
    "# 10 classes: 'airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Visualization of some pictures\n",
    "plt.figure(figsize=[12,10])\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.xlabel(class_names[y_train[i, 0]])\n",
    "    plt.imshow(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848e087-4c04-422c-af8f-9e42a0a483ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  datasets with a different types of clothes\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist \n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b6d7d-2b55-420e-9f5d-fda52abf7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset witn words and part of speeches\n",
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ]) # work for numpy == 1.23.5\n",
    "\n",
    "# Showing data\n",
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "\n",
    "\n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])\n",
    "\n",
    "\"\"\"\n",
    "NOUN\tVERB\n",
    "merger\tproposed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac68e5-589a-4f18-bf56-7a952dd7e3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d56960b-78a8-4457-b5a0-9332798cecdc",
   "metadata": {},
   "source": [
    "# 2. tensorflow  keras things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cca69-f13d-46d4-a366-b82497f3fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloling model \n",
    "model_1 = keras.models.clone_model(model)\n",
    "\n",
    "\n",
    "#  Агументации https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
    "keras.layers.RandomCrop(30, 30, 3),\n",
    "keras.layers.RandomRotation(factor=(-0.02, 0.02)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55fffd-dfc7-4cc2-afbb-bfc1ea4834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "По умолчанию keras.layers.Dense будет применяться один раз ко всем согласованным шагам времени (Dense would apply once to all time-steps concatenated). \n",
    "Мы используем keras.layers.TimeDistributed для изменения Dense слоя таким образом, \n",
    "чтобы он применялся как по пакетной, так и по временной оси.\n",
    "\"\"\"\n",
    "\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense) #This wrapper allows to apply a layer to every temporal slice of an input.\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6e1dc",
   "metadata": {},
   "source": [
    "# 3. Build a binary classification model\n",
    "\n",
    "<b> Dropout </b>\n",
    "\n",
    "Метод регуляризации искусственных нейронных сетей, предназначен для уменьшения переобучения сети за счет предотвращения сложных адаптаций отдельных нейронов на тренировочных данных во время обучения.\n",
    "\n",
    "Характеризует исключение определённого процента (например 50%) случайных нейронов на разных итерациях во время обучения нейронной сети. В результате  обучение происходит более общее, нет надежды на определенные нейроны. Такой приём значительно увеличивает скорость обучения, качество обучения на тренировочных данных, а также повышает качество предсказаний модели на новых тестовых данных.\n",
    "\n",
    "На моменте предсказания все нейроны включаются обратно, dropout не используется.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1KQrdTDanDkLhf8Kn8c5ryjqN2acwYWII' width=500>\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1j8SxKYEi12jzJXi_bPO28q5SV9emuu0Y'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aee62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# hid_size - скрытый слой, количество нейронов в промежуточных слоях.\n",
    "hid_size = 256\n",
    "\n",
    "# Шаг 1. задание Sequential - модель с линейной(полносвязной) последовательностью слоев\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        # layers.Dense - линейный полносвязный слой\n",
    "        keras.layers.Dense(\n",
    "            #hid_size - число нейронов, для первого слоя прописывается input_shape - сколько признаков придет на вход\n",
    "            hid_size, activation=\"relu\", input_shape=(train_features.shape[-1],)\n",
    "        ), # fully-connected y^1\n",
    "        keras.layers.Dense(hid_size, activation=\"relu\"), # y^2\n",
    "        \n",
    "        # Dropout - удалить 30% нейронов\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(hid_size, activation=\"relu\"), # y^3\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"), # y^4\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Шаг 2. создание списка метрик который мониториться во время обучения\n",
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Шаг 3. компиляция, включает в себя:\n",
    "## optimizer - задание оптимизатора\n",
    "## loss - функция потерь\n",
    "## список метрк \n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "\n",
    "# callbacks - позволяет хранить промежуточные чекпоинты\\сохранять ч-л\\т.п.\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\"),\n",
    "    keras.callbacks.EarlyStopping(),\n",
    "    keras.callbacks.ReduceLROnPlateau()\n",
    "]\n",
    "\n",
    "# class_weight хранит веса для разных классов\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "\n",
    "# запускаем обучение\n",
    "model.fit(\n",
    "    train_features,       # признаки для обучения X_train\n",
    "    train_targets,        # таргеты , y_train\n",
    "    batch_size=2048,      # размер бача (сколько примеров даем НС в рамках одной эпохи, т.е. все примеры делим на бачи, каждый размеом 2048)\n",
    "    epochs=30,            # количество эпох - сколько раз пройдем по ВСЕМУ дата сету ЦЕЛИКОМ\n",
    "    callbacks=callbacks,  # сохранение чекпоинтов\n",
    "    validation_data=(val_features, val_targets),   # данные для валидации , turple (x_val, y_val)\n",
    "    class_weight=class_weight,     # для балансировки классов\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a223a",
   "metadata": {},
   "source": [
    "# 4. Нелнейные НС\n",
    "Однако на практике не все модели удается реализовать с помощью keras.Sequential.\n",
    "\n",
    "Если в модели присутствует \"нелинейная\" структура (т.е. есть разветвление потока данных), то ```keras.Sequential()``` не подойдет для построения такой модели. На помощь приходит инструмент *functional API*.\n",
    "\n",
    "\n",
    "Рассмотрим следующую модель:\n",
    "\n",
    "```\n",
    "(input: 784-dimensional vectors)\n",
    "    \n",
    "       ↧\n",
    "    \n",
    "[Dense (32 units, relu activation)]\n",
    "    \n",
    "       ↧\n",
    "    \n",
    "[Dense (32 units, relu activation)]\n",
    "    \n",
    "       ↧\n",
    "    \n",
    "[Dense (10 units, softmax activation)]\n",
    "    \n",
    "       ↧\n",
    "    \n",
    "(output: logits of a probability distribution over 10 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c13cb",
   "metadata": {},
   "source": [
    "\n",
    "Обычно в глубоком обучении принято использовать следующую структуру во входных данных:\n",
    "\n",
    "$$[n, N_{ch}, W,H],$$\n",
    "\n",
    "где $n$ - размер подвыборки (batch_size), \n",
    "\n",
    "$N_{ch}$ - кол-во каналов (красный, зеленый, синий), \n",
    "\n",
    "$W$, $H$ - ширина и высота изображения. \n",
    "\n",
    "\n",
    "Однако batch_size не указывается при объявлении слоев в Keras. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input() используется для создания экземпляра тензора Кераса.\n",
    "#Тензор Кераса - это символический тензорный объект, который мы дополняем определенными атрибутами, позволяющими нам построить модель Кераса, просто зная входы и выходы модели.\n",
    "\n",
    "img_inputs = keras.Input(shape=(32, 32, 3))\n",
    "print(img_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a8d27",
   "metadata": {},
   "source": [
    "\n",
    "На сегодняшний день мы пока забудем о вышеописанной структуре и будем все входные данные трактовать как один длинный вектор длины  $(N_{ch}\\cdot W \\cdot H)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,))\n",
    "\n",
    "print(\"Shape of input is\", inputs.shape)\n",
    "print(\"Data type of input is\", inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Следующий слой, который будет использовать тензор inputs как входные данные, должен быть объявлен следующим образом:\n",
    "#  создание нелинейной НС\n",
    "## тут создаются две ветви: x и y. Далее из них будет формироватсья финальное предсказание.\n",
    "\n",
    "\n",
    "### шаг 1. объявление слоя x\n",
    "x = layers.Dense(32)(inputs)        #создание слоя Dense x, на входе inputs, на выходе 32 нейрона\n",
    "x = layers.BatchNormalization()(x)  #нормализация слоя\n",
    "x = layers.Dropout(0.5)(x)          #Dropout \n",
    "x = layers.Activation('relu')(x)    #фун активации Relu\n",
    "\n",
    "# еще один полносвязный слой с 32 нейронами и фун. активации Relu\n",
    "dense21 = layers.Dense(32, activation=\"relu\")(x)   \n",
    "\n",
    "\n",
    "### Шаг 2. объявление слоя y (по сути зеркален х)\n",
    "y = layers.Dense(32)(inputs)       #создание слоя Dense y, на входе inputs\n",
    "y = layers.BatchNormalization()(y)\n",
    "y = layers.Dropout(0.5)(y)\n",
    "y = layers.Activation('relu')(y)\n",
    "\n",
    "dense22 = layers.Dense(32, activation='relu')(y)\n",
    "\n",
    "\n",
    "# Шаг 3. суммирование тензоров\n",
    "dense3 = layers.Add()([dense21, dense22])\n",
    "\n",
    "\n",
    "# Шаг 4. еще парочку полносвязных слоев\n",
    "z = layers.Dense(32, activation=\"relu\")(dense3) # слой с 32 нейронами на выходе и на входе dense3\n",
    "outputs = layers.Dense(10)(z)  # выходной слой с 10 нейронами (напирмер для классификации 10 рукописных цифр)\n",
    "\n",
    "\n",
    "# Шаг 5. На этом этапе мы можем закончить добавление новых блоков и объявить модель, которая будет состоять из данных слоёв.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"our_first_model\")\n",
    "model.summary()   # инфа по модели\n",
    "keras.utils.plot_model(model, \"my_first_model.png\") # нарисовать модель в виде направленного графа.\n",
    "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)  # нарисовать модель в виде направленного графа с указанием shapes.\n",
    "\n",
    "\n",
    "# Шаг 6. Подготовка данных\n",
    "## загрузка датасета\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "## Нормализация картинок, перевод яркости из значения [0,255] -> [0,1]\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255.\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255.\n",
    "\n",
    "\n",
    "# Шаг 7. Компиляция, включает в себя:\n",
    "## optimizer - задание оптимизатора\n",
    "## loss - функция потерь\n",
    "## список метрк \n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), ### from_logits - а-ля софтмакс вшитый SparseCategoricalCrossentropy (который не указан при инициализации outputs)\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Шаг 8. Обучение модели\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Шаг 9. Предсказание и результаты\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "\n",
    "\n",
    "# Шаг 10. Сохранение модели. \n",
    "## Без callback сохраняется посл. эпоха.\n",
    "## Также читай про EarlyStopping https://keras.io/api/callbacks/early_stopping/\n",
    "\"\"\"\n",
    "model.save() при вызове сохраняет модель в один файл. \n",
    "Сохраненный файл включает в себя:\n",
    "-архитектуру модели;\n",
    "-веса модели;\n",
    "-config-file (если такой был создан при компиляции);\n",
    "-оптимизатор и его состояние.\n",
    "\"\"\"\n",
    "model.save(\"path_to_my_model\")\n",
    "del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"path_to_my_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4379c-d6f7-43dc-8725-3e8e951110f4",
   "metadata": {},
   "source": [
    "# 5. Сверточные НС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d4f5a-7ba8-47b7-bbc4-1f7c05ef16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение выборки на обучение и ВАЛИДАЦИЮ\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_val, x_train, x_val = train_test_split(\n",
    "    train_label, train_img, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Шаг 1. Задаем модель. \n",
    "# Sequential - линейная последовательность слоев\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    ### Этап 1 - feature selection\n",
    "    # 1ый слой: Сверточный слой с filters(количеством фильтров), kernel_size(размер сверточного фильтра), padding='same'(на входе 28х28 и на выходе 28х28)\n",
    "    # т.к. это первый слой , указываем размер входного тензора input_shape\n",
    "    tf.keras.layers.Conv2D(filters=6,\n",
    "                           kernel_size=(5, 5),\n",
    "                           padding='same',\n",
    "                           activation='relu',\n",
    "                           input_shape=x_train.shape[1:]),\n",
    "    \n",
    "    # 2ой слой: Пулинг, с разбивкой на сегменты 2х2 (pool_size); padding='valid' - паддинг не добавляется. На выходе 14х14х6(число фильтров)\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='valid'),\n",
    "    \n",
    "    # 3ий слой:  Сверточный слой с filters(количеством фильтров) = 16 (стало больше)\n",
    "    tf.keras.layers.Conv2D(filters=16, \n",
    "                           kernel_size=(5, 5),\n",
    "                           padding='same',\n",
    "                           activation='relu'),\n",
    "    \n",
    "    # 4ыой слой: Пулинг\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='valid'),\n",
    "    \n",
    "    ### Этап 2 - перцептрон\n",
    "    #Вытягивание в вектор: на входе (N,C,H,W) -> на выходе (N, C*H*W)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    #Полносвязный слой с relu\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    \n",
    "    #Полносвязный финальный слой с softmax\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "\n",
    "# Шаг 2. Компиляция модели с указанием оптимизатора, функции потерь и метрики качества.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',   # or 'sparse_categorical_crossentropy' when there isn't one-hot coding\n",
    "              metrics=['accuracy'])              # or 'sparse_categorical_accuracy' when there isn't one-hot coding\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# Total params: N, большнство из которых в dense (последних слоях), тогда как сверточный слов содержит сильно меньше M . N >> M\n",
    "\n",
    "\n",
    "# Шаг 3. По необходиомсти перевод лейблов в categorical (One-hot кодировка разметки) (или используй sparse_categorical_)\n",
    "#y_train, y_val = (keras.utils.to_categorical(y) for y in (y_train, y_val))\n",
    "y_train_labels = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "\n",
    "# Шаг 4. Обучение\n",
    "model.fit(x_train, \n",
    "          y_train_labels,\n",
    "          batch_size=32, \n",
    "          epochs=5,\n",
    "          validation_split=0.2)\n",
    "\n",
    "\n",
    "# Шаг 5. Модель выдает 10 вероятностей к каждому классу. Используем argmax для получения индекса максимальной веротности.\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# Шаг 6. Оцениваем качество решение на валидационной выборке\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %s' % accuracy_score(y_val, y_pred_labels))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_pred_labels))\n",
    "\n",
    "\n",
    "\n",
    "# Шаг 7. Формирование файлика со всеми предсказаниями на тестовой выборке.\n",
    "with open('submit.txt', 'w') as dst:\n",
    "    dst.write('ImageId,Label\\n')\n",
    "    for i, p in enumerate(y_pred_test_labels, 1):\n",
    "        dst.write('%s,%d\\n' % (i, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729da16-cbb9-4a07-8391-79ad132cb017",
   "metadata": {},
   "source": [
    "# 6. Рекурентные НС\n",
    "также смотри ipunb  файл Введение в рекурентные сети и github 02_rnn.ipynb\n",
    "\n",
    "была задача по описаниях вакансий предсказывать уровень их зарплат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8879c-f9b2-4b63-8a1b-5102c664e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# различные вспомогательные функции\n",
    "\n",
    "# map() -  когда нужно применить функцию преобразования к каждому элементу в коллекции или в массиве и преобразовать их в новый массив.\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "# функция переводит набор текстовых последовательностей в матрицу (тензор)\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" \n",
    "    Convert a list of tokens into a matrix with padding \n",
    "\n",
    "    Lines:\n",
    "    engineering systems analyst\n",
    "    Matrix:\n",
    "    [[ 624 1701  133]]\n",
    "    \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):  # enumerate() – сразу индекс элемента и его значение.\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "# функция создания батча: упаковка текстов и категориальных признаков для передачи в нейронку\n",
    "def make_batch(data, max_len=None, word_dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {} # содержит по ключам различные модальности\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len) # батч тайтл содержит тензор с тайтлами\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len) # батч FullDescription содержит тензор с описаниями\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values  # в целевую колонку передаем живые значения\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "# хз что это такое \n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])  # Генерирует случайную выборку из заданного 1-D массива\n",
    "    dropout_mask &= matrix != pad_ix  # создание некой маски удаления элементов\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])\n",
    "\n",
    "\n",
    "\n",
    "# функция которая строит, компилирует и возвращает модель\n",
    "## n_tokens - всего токенов в тексте\n",
    "## n_cat_features - количество категориальных признаков\n",
    "def build_model(n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "    \"\"\" Построение модели, отображающей три источника данных в единый линейный выход: предсказанное значение \"\"\"\n",
    "    \n",
    "    # задаем три входа\n",
    "    l_title = L.Input(shape=[None], name=\"Title\")  #shape пока неизвестен , т.к. будет меняться\n",
    "    l_descr = L.Input(shape=[None], name=\"FullDescription\")\n",
    "    l_categ = L.Input(shape=[n_cat_features], name=\"Categorical\")\n",
    "    \n",
    "    # задаем слой embedding, который будет обрабатывать и title и description (имеет смысл задать один эмбеддинг т.к. слова в title и вуыскшзешщт могут пересекаться оч сильно)\n",
    "    emb = L.Embedding(n_tokens, 2 * hid_size)\n",
    "    \n",
    "    l_title_emb = emb(l_title)\n",
    "    l_descr_emb = emb(l_descr)\n",
    "    \n",
    "    \n",
    "    ## embedding передаем в сверточные слои, тут одномерная свертка\n",
    "    ## kernel_size = 2 или 5 => аггрегируем информацию с 2 или 5 последовательных слов\n",
    "    l_title_conv = L.Convolution1D(hid_size, kernel_size=2, activation='relu')(l_title_emb)\n",
    "    l_descr_conv = L.Convolution1D(hid_size, kernel_size=5, activation='relu')(l_descr_emb)\n",
    "    \n",
    "    \n",
    "    ## Max Pooling \n",
    "    l_title_out = L.GlobalMaxPool1D()(l_title_conv)\n",
    "    l_descr_out = L.GlobalMaxPool1D()(l_descr_conv)\n",
    "    \n",
    "    # Категориальные признаки обрабатываем с помощью линейного слоя\n",
    "    l_categ_out = L.Dense(hid_size, activation='relu')(l_categ)\n",
    "    \n",
    "    # получаем выходы трех ветвей l_title_out, l_descr_out и l_categ_out - которые нужно объединить\n",
    "    \n",
    "    # Объединяем с помощью Concatenate\n",
    "    l_combined = L.Concatenate()([l_title_out, l_descr_out, l_categ_out])\n",
    "    \n",
    "    # Комбинированный тензор передаем в многослойный перцептрон\n",
    "    l_dense_clf = L.Dense(hid_size, activation='relu')(l_combined)\n",
    "    \n",
    "    \n",
    "    output_layer = L.Dense(1)(l_dense_clf)\n",
    "    # end of your code\n",
    "    \n",
    "    \n",
    "    # Объединение всего с помощью models.Model\n",
    "    model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "    \n",
    "    # Компиляция модели\n",
    "    model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# функция которая нужна для кастомного load - которая из csv вытягиваем бачи\n",
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
    "            target = batch.pop(target_column)\n",
    "            yield batch, target\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9686d-10b1-47a6-be2b-b67ba30fda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постоение модели и вывод основной информации о ней\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# Всего параметров более N млн\n",
    "# Больше всего весов на слое эмбедингов - тоже более N млн, т.к. слой эмбединг для каждого индекса сопостовлят вектор\n",
    "\n",
    "\n",
    "dummy_pred = model.predict(make_batch(data_train[:100]))\n",
    "dummy_loss = model.train_on_batch(make_batch(data_train[:100]), data_train['Log1pSalary'][:100])[0]\n",
    "\n",
    "    \n",
    "batch_size = 256\n",
    "epochs = 10            # definitely too small\n",
    "steps_per_epoch = 100  # for full pass over data: (len(data_train) - 1) // batch_size + 1\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "#Теперь мы можем подогнать нашу модель обычным способом. Интересно то, что мы тренируемся на бесконечном потоке мини-пакетов, производимых функцией iterate_minibatches.\n",
    "model.fit_generator(iterate_minibatches(data_train, batch_size, cycle=True, word_dropout=0.05), \n",
    "                    epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                    \n",
    "                    validation_data=iterate_minibatches(data_val, batch_size, cycle=True),\n",
    "                    validation_steps=data_val.shape[0] // batch_size\n",
    "                   )\n",
    "\n",
    "\n",
    "def print_metrics(model, data, batch_size=batch_size, name=\"\", **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    for batch_x, batch_y in iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw):\n",
    "        batch_pred = model.predict(batch_x)[:, 0]\n",
    "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
    "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
    "        num_samples += len(batch_y)\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
    "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
    "    return squared_error, abs_error\n",
    "    \n",
    "print_metrics(model, data_train, name='Train')\n",
    "print_metrics(model, data_val, name='Val');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce34ec5-8cab-4a1d-9488-dcdb112aa54e",
   "metadata": {},
   "source": [
    "<b>Bonus part: explaining model predictions</b>\n",
    "\n",
    "данный код помогает определить какие именно слова больше всего повлияли на предсказание.\n",
    "\n",
    "It's usually a good idea to understand how your model works before you let it make actual decisions. It's simple for linear models: just see which words learned positive or negative weights. However, its much harder for neural networks that learn complex nonlinear dependencies.\n",
    "\n",
    "There are, however, some ways to look inside the black box:\n",
    "* Seeing how model responds to input perturbations\n",
    "* Finding inputs that maximize/minimize activation of some chosen neurons (_read more [on distill.pub](https://distill.pub/2018/building-blocks/)_)\n",
    "* Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)\n",
    "\n",
    "Today we gonna try the first method just because it's the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b5548-4e59-4ba0-b46a-2a544d119a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(model, sample, col_name='Title'):\n",
    "    \"\"\" Computes the effect each word had on model predictions \"\"\"\n",
    "    sample = dict(sample)\n",
    "    sample_col_tokens = [tokens[token_to_id.get(tok, 0)] for tok in sample[col_name].split()]\n",
    "    data_drop_one_token = pd.DataFrame([sample] * (len(sample_col_tokens) + 1))\n",
    "\n",
    "    for drop_i in range(len(sample_col_tokens)):\n",
    "        data_drop_one_token.loc[drop_i, col_name] = ' '.join(UNK if i == drop_i else tok\n",
    "                                                   for i, tok in enumerate(sample_col_tokens)) \n",
    "\n",
    "    *predictions_drop_one_token, baseline_pred = model.predict(make_batch(data_drop_one_token))[:, 0]\n",
    "    diffs = baseline_pred - predictions_drop_one_token\n",
    "    return list(zip(sample_col_tokens, diffs))\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display_html\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html\n",
    "\n",
    "\n",
    "\n",
    "i = 36605\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);\n",
    "\n",
    "\n",
    "\n",
    "i = 12077\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);\n",
    "\n",
    "\n",
    "i = np.random.randint(len(data))\n",
    "print(\"Index:\", i)\n",
    "print(\"Salary (gbp):\", np.expm1(model.predict(make_batch(data.iloc[i: i+1]))[0, 0]))\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1648cc-4429-4119-8549-bfbd0afa8161",
   "metadata": {},
   "source": [
    "# 7 Evaluator - позволяет автоматизированно компилировать и получать скоры разных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01456119-ed89-4c05-99d1-878b5955e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class Evaluator: child class from list\n",
    "class Evaluator(list):\n",
    "    \n",
    "    ## Constuctor\n",
    "    def __init__(self, models, optimizers='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=[keras.metrics.sparse_categorical_accuracy]):\n",
    "        '''\n",
    "            models: dict {name: model}\n",
    "            optimizers: list of optimizers or just one optimizer\n",
    "        '''\n",
    "        \n",
    "        if not isinstance(models, dict):  # checking: object [models] is instance of class dict ? or not\n",
    "            models = {'single_model': models}\n",
    "            \n",
    "        if not isinstance(optimizers, dict):\n",
    "            optimizers = {str(optimizers.__class__): optimizers}\n",
    "            \n",
    "        \n",
    "        # initing an object list, where each element is turple: (Model Name, Copy of Model, Optimizer Name, Optimizer Object)\n",
    "        super().__init__( \n",
    "                            [\n",
    "                                (model_name, keras.models.clone_model(model), optimizer_name, optimizer)\n",
    "                                for model_name, model in models.items()\n",
    "                                for optimizer_name, optimizer in optimizers.items()\n",
    "                            ]\n",
    "                        )\n",
    "        \n",
    "        #Compiling of models\n",
    "        for _, model, _, optimizer in self:\n",
    "            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    \n",
    "    # Education. writing logs, construct graphics (epochs, metric value)\n",
    "    def fit(self, X, y, validation_data=(), max_epochs=100, verbose=0, callbacks=[], batch_size=32, tensorboard=True):\n",
    "        if not isinstance(callbacks, list):\n",
    "            callbacks = [callbacks]\n",
    "        \n",
    "        histories = []   \n",
    "        \n",
    "        for model_name, model, optimizer_name, optimizer in tqdm_notebook(self):\n",
    "            history = model.fit(\n",
    "                              X, \n",
    "                              y, \n",
    "                              validation_data=validation_data or None, \n",
    "                              epochs=max_epochs, \n",
    "                              verbose=verbose,\n",
    "                              batch_size=batch_size, \n",
    "                              callbacks=callbacks \n",
    "                                        + [keras.callbacks.TensorBoard(log_dir=os.path.join(os.getcwd(), 'logs\\\\{}_{}'.format(model_name, optimizer_name)))] if tensorboard else []\n",
    "                             )\n",
    "            histories.append((history, model_name, optimizer_name))\n",
    "            \n",
    "            \n",
    "        # Visualization using History\n",
    "        plt.figure(figsize=[18,12])\n",
    "        for i in range(len(histories)):\n",
    "            plt.subplot(len(histories)//3, 3, i + 1)\n",
    "            plt.plot(histories[i][0].history['sparse_categorical_accuracy'], label='test')\n",
    "            plt.plot(histories[i][0].history['val_sparse_categorical_accuracy'], label='validation')\n",
    "            plt.legend()\n",
    "            plt.xlabel(f'{histories[i][1]}:{histories[i][2]}')\n",
    "            plt.ylabel('Accuracy')\n",
    "        \n",
    "\n",
    "        \n",
    "    # ???\n",
    "    def fit_generator(self, X, y, validation_data=(), max_epochs=100, verbose=1, callbacks=[], batch_size=32):\n",
    "        datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "        \n",
    "        if not isinstance(callbacks, list):\n",
    "            callbacks = [callbacks]\n",
    "            \n",
    "        for model_name, model, optimizer_name, optimizer in tqdm_notebook(self):\n",
    "            model.fit_generator(datagen.flow(\n",
    "                                             X, \n",
    "                                             y, \n",
    "                                             batch_size=batch_size\n",
    "                                            ), \n",
    "                                epochs=max_epochs,\n",
    "                                validation_data=validation_data or None, \n",
    "                                verbose=verbose,\n",
    "                                callbacks=callbacks + [keras.callbacks.TensorBoard(log_dir=os.path.join(os.getcwd(), 'logs\\\\{}_{}'.format(model_name, optimizer_name)))]\n",
    "                               )\n",
    "\n",
    "    \n",
    "    # Final score\n",
    "    def evaluate(self, X, y, metric):\n",
    "        for model_name, model, optimizer_name, _ in self:\n",
    "            print('Final score of {}_{} is {}'.format(model_name, optimizer_name,\n",
    "                  metric(y_test, np.argmax(model.predict(X_test), axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb3a1f-047c-4975-bd35-bbe638d8c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = { \n",
    "                'SGD': keras.optimizers.legacy.SGD(),  \n",
    "                'Adam': keras.optimizers.legacy.Adam(),  \n",
    "                'RMSprop': keras.optimizers.legacy.RMSprop()\n",
    "              }\n",
    "\n",
    "\n",
    "models = { \n",
    "                'tiny': model_1,\n",
    "                'conv': model_2\n",
    "              }\n",
    "\n",
    "\n",
    "evaluator = Evaluator(models, optimizers=optimizers)\n",
    "evaluator.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
    "evaluator.evaluate(X_test, y_test, accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45731fb9-682a-4292-a415-25a6603ac515",
   "metadata": {},
   "source": [
    "# 8. Models - класс для построения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f540ac-7b89-43d6-a4c4-d0ad6012d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !!! important about BatchNormalization\n",
    "## The general use case is to use BN between the linear and non-linear layers in your network, \n",
    "## because it normalizes the input to your activation function\n",
    "\n",
    "## class Models: child from OrderedDict. The class might add a new layer to a model\n",
    "class Models(OrderedDict):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, models):\n",
    "        if not isinstance(models, dict):\n",
    "            models = OrderedDict({'base_model': models})\n",
    "        super().__init__(models)\n",
    "\n",
    "    # Add a new layer to a every model in dict\n",
    "    def add(self, layer):\n",
    "        for name, model in self.items():\n",
    "            model.add(layer)  # add layers to the end of model\n",
    "\n",
    "    \n",
    "    # create model \"name\" by updating first element (base model)\n",
    "    def add_create(self, name, layer):\n",
    "        base_model = next(iter(self.items()))[1]\n",
    "        new_model = keras.models.clone_model(base_model)\n",
    "        new_model.add(layer)\n",
    "        self.update({name: new_model})\n",
    "\n",
    "    # create model \"name\" by updating last element (some new model)\n",
    "    def add_update(self, name, layer):\n",
    "        base_model = self[next(reversed(self))] # reversed - inverse iter()\n",
    "        new_model = keras.models.clone_model(base_model)\n",
    "        new_model.add(layer)\n",
    "        self.update({name: new_model})\n",
    "        \n",
    "\n",
    "## !!! important about BatchNormalization\n",
    "## The general use case is to use BN between the linear and non-linear layers in your network, \n",
    "## because it normalizes the input to your activation function\n",
    "        \n",
    "       \n",
    "models = Models(keras.Sequential())                                        # create DictOrder with empty linear model\n",
    "models.add(keras.layers.InputLayer(input_shape=X_train.shape[1:]))               # InputLayer - entry point into a Network    \n",
    "models.add(keras.layers.Convolution2D(filters=10, kernel_size=(3, 3)))     # Convolution2D is the same Conv2D\n",
    "models.add(keras.layers.MaxPooling2D(pool_size=(2,2), padding='valid'))    # MaxPooling2D is the same MaxPool2D\n",
    "models.add_create('conv_batchnorm', keras.layers.BatchNormalization())     # Adding BatchNormalization\n",
    "models.add(keras.layers.Activation('relu'))\n",
    "models.add(keras.layers.Flatten())\n",
    "models.add(keras.layers.Dense(100))\n",
    "models.add_update('fully_conn_batchnorm', keras.layers.BatchNormalization())   # Adding BatchNormalization\n",
    "models.add(keras.layers.Activation('relu'))\n",
    "models.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a98c3-abd7-4c4b-bd1c-93830bf6b143",
   "metadata": {},
   "source": [
    "# 9.  Autoencoders (Автоэнкодер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8d454-527d-4df9-8cf6-0dde1fb67e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###Step 1. модель простейшего (обычного) автоэнкодера\n",
    "encoding_dim = 2 # будем сжимать картинки до 2х координат\n",
    "\n",
    "# входной слов\n",
    "input_img = keras.Input(shape=(784,)) \n",
    "\n",
    "# часть энкодэра\n",
    "encoded1 = layers.Dense(64, activation='relu')(input_img)  # снижение размерности до 64\n",
    "encoded2 = layers.Dense(encoding_dim, activation='relu')(encoded1)  # снижение размерности до encoding_dim\n",
    "\n",
    "# часть декодэера (зеркален энкодэру)\n",
    "decoded2 = layers.Dense(64, activation='relu')(encoded2) # увеличение размерности до 64\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded2) # sigmoid - т.к. выход модели был от 0 до 1\n",
    "\n",
    "# создание autoencoder\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "\n",
    "# Step 2. отдельно прописывается энкодэр\n",
    "encoder = keras.Model(input_img, encoded2) # input_img вход остается таким же ; выход - это слой encoded2\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "\n",
    "\n",
    "# отдельно прописывается декодэр (обращение по индексам)\n",
    "decoder_layer1 = autoencoder.layers[-1]\n",
    "decoder_layer2 = autoencoder.layers[-2]\n",
    "decoder = keras.Model(encoded_input, decoder_layer1(decoder_layer2(encoded_input)))\n",
    "\n",
    "\n",
    "# Step 3. компилируем модель\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "# Step 4. Data and datatransforming \n",
    "\n",
    "# загрузка датасета mnist и разбивка на трейн и тест\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# нормализация данных; вытягивание картинок в длинный вектор\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) # np.prod - Возвращает произведение элементов массива\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "\n",
    "# Step 5. запуск обучения; \n",
    "# тут по x_train как бы предсказываем x_train, т.к. нам нужны исхожные картинки в результате; \n",
    "# shuffle - перетасовывать данные обучения перед каждой эпохой\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "\n",
    "## Step 6. проверка на сколько хорошо автодекодэр справляется с задачей кодировки и декодировки изображений, для этого\n",
    "# 1. сначала кодируем ячейки из тестового датасета\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "# 2. далее декодираем ячейки из тестового датасета, чтоб посмотреть на сколько хорошо восстановлено первоначальное изображение\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "\n",
    "# Step 7. проходим по тестовому датасету, изображаем сначала исходное а затем снизу декодированние изображение\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 8. Отображение двумерного многообразия цифр\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Мы выберем n точек в пределах [-15, 15] стандартных отклонений.\n",
    "grid_x = np.linspace(-10, 10, n)\n",
    "grid_y = np.linspace(-10, 10, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3939c9a-70c1-4707-b5c4-3f55a1667c24",
   "metadata": {},
   "source": [
    "# 10. Variational autoencoder (Вариационный автоэнкодер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6a66e-0055-4791-9d14-8524a061270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Инициализация слоев\n",
    "original_dim = 28 * 28  #\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "\n",
    "inputs = keras.Input(shape=(784,))\n",
    "h = layers.Dense(intermediate_dim, activation='relu')(inputs) # понижение размерности до 64\n",
    "z_mean = layers.Dense(2)(h)  # слой для создания вектора средних\n",
    "z_log_sigma = layers.Dense(2)(h) # слой для создания вектора стандартных отклонений\n",
    "\n",
    "\n",
    "# Step 2. parametrization trick - логика сэмплирования векторов z_mean и z_log_sigma: N(mu, sigma) ---> mu + eps*sigma, eps=N(0,1)\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = keras.backend.random_normal(shape=(keras.backend.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + keras.backend.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma]) # Lambda - кастомный слой через который передаем логику слоя\n",
    "\n",
    "\n",
    "# Step 3.\n",
    "# Create encoder: на вход инпуты, на выходе z_mean, z_log_sigma, z\n",
    "vae_encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='vae_encoder')\n",
    "\n",
    "# Create decoder: на выходе вектор Z, далее его повышение до 64 и до original_dim\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "vae_decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = vae_decoder(vae_encoder(inputs)[2]) # применили инкодер до 2х мерного и передали его в декодер\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "\n",
    "# Step 4. будем использовать две функции потерь\n",
    "# 1ое для основной задачи\n",
    "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "# 2ое kl_loss для оценки на сколько два распределния похожи друг на друга: чтобы z_mean -> 0 и z_log_sigma -> 1\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# комбинация двух функций потерь как срежнее между ними\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# добавление vae_loss к модели\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "\n",
    "\n",
    "# Step 5. получение train test\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# нормализация и вытягивание в вектор\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# обучение\n",
    "hist = vae.fit(x_train, x_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, x_test),\n",
    "        workers=-1,\n",
    "        shuffle=True)\n",
    "\n",
    "\n",
    "# Step 6. кодирование изображения и получения ее обратно\n",
    "vae_encoded_imgs = vae_encoder.predict(x_test)\n",
    "decoded_imgs = vae_decoder.predict(vae_encoded_imgs[2])\n",
    "\n",
    "\n",
    "# Step 7. проходим по тестовому датасету, изображаем сначала исходное а затем снизу декодированние изображение\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Отображение двумерного многообразия цифр\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# We will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = vae_decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa0076-01a0-41f4-bc6f-ab262ec1f8dc",
   "metadata": {},
   "source": [
    "# 11. Инструкция по Tensorboard\n",
    "\n",
    "TensorBoard позволяет логгировать процесс обучения в реальном времени с построением графиков и просмотром промежуточных результатов. Его можно интегрировать как в jupyter notebook на локальной машине, так и в Google Colab.\n",
    "\n",
    "Для запуска на локальной машине необходимо сначала установить TensorBoard:\n",
    "\n",
    "pip install tensorboard\n",
    "\n",
    "Затем его надо запустить:\n",
    "\n",
    "tensorboard --logdir YOUR_DIRECTORY_HERE\n",
    "\n",
    "При успешном запуске доступ к TensorBoard осуществляется через порт 6006 (это можно изменить, вызвав аргумент --port).\n",
    "\n",
    "Далее доступ осуществляется через браузер (localhost:6006)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91867a43-b4c9-462e-bcc4-cd9f2eee75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the TensorBoard notebook extension for GoogleColab\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "### for local \n",
    "pip install tensorboard\n",
    "tensorboard --logdir YOUR_DIRECTORY_HERE  # jupiter\n",
    "localhost:6006   # from browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e9ae2-4e5f-4e18-a559-175ed2774e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пример\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "# Step 1. Load and split data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist # datasets with a different types of clothes\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "# Step 2. Creating model\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "\n",
    "# Step 3. Creating, compiling and fitting model. Callbacks contain a tf.keras.callbacks.TensorBoard\n",
    "def train_model():\n",
    "  \n",
    "    model = create_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # directory where logs will be stored\n",
    "    #logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    #logdir = os.path.join(os.getcwd(), 'logs\\\\{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    logdir = 'C:\\\\Users\\\\e.radionov\\\\Netology ML Course\\\\logs\\\\{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))  # траблы когда в пути кириллица\n",
    "    print(logdir)\n",
    "    # init TensorBoard\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir, histogram_freq=1)\n",
    "\n",
    "    model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=5, \n",
    "            validation_data=(x_test, y_test), \n",
    "            callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580cc6d-611c-4aa8-a49a-4ce0c554489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Теперь мы можем в реальном времени (почти) следить за процессом обучения..\n",
    "%tensorboard --logdir logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be0b53-3a22-44cc-af10-3071cdbf7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab139f-1581-457d-bcf7-2212d5b56313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Теперь мы можем в реальном времени (почти) следить за процессом обучения..\n",
    "%tensorboard --logdir logs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3badb-84f8-4368-a7bd-38b2cb64103f",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
