{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d810485",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Menu\n",
    "- <b> 1. Libraries </b>\n",
    "- <b> 2. Pandas </b>\n",
    "- <b> 3. Numpy </b>\n",
    "- <b> 4. seaborn </b>\n",
    "- <b> 5. matplotlib </b>\n",
    "- <b> 6. preprocessing (PolynomialFeatures, LabelEncoder, Normalizing, imblearn) </b>\n",
    "- <b> 7. SKLearn - (pipeline, train_test_split, plot_tree) </b>\n",
    "- <b> 8. SKLearn ML Models </b>\n",
    "- <b> 9. SKLearn Ansambles: Бэггинг, Стекинг и Boosting </b>\n",
    "- <b> 10. SKLearn Outliers Detecting (Emissions): OneClassSVM, IsolationForest и DBSCAN etc </b>\n",
    "- <b> 11. Feature Selection и методы Уменьшения размерности </b>\n",
    "- <b> 12. Алгоритм градиентного спуска </b>\n",
    "- <b> 13. Алгоритм стохастического градиентного спуска </b>\n",
    "- <b> 14. Accuracy, precision, recall and ROC AUC and PR AUC </b>\n",
    "- <b> 15. L1 L2 Regularizations </b>\n",
    "- <b> 16. Функция работы с пропусками </b>\n",
    "- <b> 17. Функия обучающая модель (для оценки разных преобразований) </b>\n",
    "- <b> 18. Функции One-Hot, Label and Count encodings </b>\n",
    "- <b> 19. Работа с геоданными (DBSCAN, KMeans, reverse_geocoder, folium, keplergl)</b>\n",
    "- <b> 20. Реализация дерева решений</b>\n",
    "- <b> 21. Кластеры </b>\n",
    "- <b> 22. Байесовская Оптимизаци </b>\n",
    "- <b> 23. Работа с текстом (CountVectorizer, Tf-Idf, word2vec, fastText) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea02468",
   "metadata": {},
   "source": [
    "# 1. Libraries, imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceb19bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 basic\n",
    "import pandas as pd # Для работы с данными\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  # Библиотека для визуализации результатов\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffb62d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_12468/4157659777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msort_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set an element by that will be a sorting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msorted_zip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_poly_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_poly_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msorted_zip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "## 2 some extra special\n",
    "import operator #is a built-in module providing a set of convenient operators. \n",
    "\n",
    "sort_axis = operator.itemgetter(0) # set an element by that will be a sorting\n",
    "sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)\n",
    "x_, y_poly_pred = zip(*sorted_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 scipy \n",
    "import scipy.stats # Statictic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3d9079-5fa1-45ac-969c-6c79d95a7b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({9: 3, 7: 2})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counter в заданном списке просто подсчитывает количество вхождений каждого элемента и возвращает результат в виде словаря.\n",
    "from collections import Counter\n",
    "\n",
    "Counter([9,9,9,7,7])\n",
    "\n",
    "\n",
    "word_counts = Counter()\n",
    "word_counts.update(words)  - добавить данные и пересчитать их встречаемость \n",
    "word_counts.most_common(10000)\n",
    "\n",
    "\n",
    "# так можно посчитать на сколько самые популярные слова в коллекции покрывают все слова в коллекции\n",
    "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n",
    "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb331516-46ee-4c7c-a425-94ae2adfbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests \n",
    "assert np.shape(X_train) == (40000, 32, 32, 3),\"data shape should not change\"\n",
    "assert 0.9 <= max(map(np.max, (X_train, X_val, X_test))) <= 1.05\n",
    "assert 0.0 <= min(map(np.min, (X_train, X_val, X_test))) <= 0.1\n",
    "assert len(np.unique(X_test / 255.)) > 10, \"make sure you casted data to float type\"\n",
    "\n",
    "\n",
    "# map python - execute function (np.max) for each element ((X_train, X_val, X_test))\n",
    "map(np.max, (X_train, X_val, X_test)) \n",
    "list(map(np.min, (X_train, X_val, X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3b2f4-d0e6-436e-9c66-2411ba079224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate() – сразу индекс элемента и его значение. # словарь token to id \n",
    "token_to_id = {token: id for id, token in enumerate(tokens)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ac9e3-c552-4775-a7c2-ab0468e5f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# изобразить таблицу html , две строки и произвольное количество столбцов\n",
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "\n",
    "\n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482d1b9-d8a2-4adb-a37f-df1fd6002186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdict - такой же dict, но первым параметром принимает какое значение присвоить по умолчанию новому (неприсутствущему в коллекции) элементу\n",
    "from collections import defaultdict\n",
    "\n",
    "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) }) \n",
    "# если вызвать word_to_id['ЛЮБОЕ_НОВОЕ_СЛОВО'], то defaultdict вызовет для него lambda:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117e57e",
   "metadata": {},
   "source": [
    "# 2. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ececfd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator.itemgetter(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PANDAS basic\n",
    "data = pd.read_csv('')\n",
    "\n",
    "data.head()\n",
    "data.describe()\n",
    "data.info()\n",
    "data.shape\n",
    "\n",
    "data['height'].unique()\n",
    "\n",
    "selectedColumns['sport'].value_counts(dropna=False)  # dropna - Don’t include counts of NaN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538ad02-3e0a-4a11-8031-92bff7af233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Корреляция между элементами .corr()   (также смотри sns.pairplot)\n",
    "corr = dataframe.corr()\n",
    "\n",
    "#отображение на графике\n",
    "plt.figure(figsize=(12,9))\n",
    "sns.heatmap(corr,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas selecting columns\n",
    "X = data[['Mileage', 'Price']] #certain columns\n",
    "X = data.loc[:, ['Mileage', 'Liter']]\n",
    "\n",
    "#loc\n",
    "X.loc[X[X[column].isna()].index ,column] = value # заполнение значением\n",
    "X['ind_'+str(column)] = 0\n",
    "X.loc[X[X[column].isna()].index, 'ind_'+str(column)] = 1\n",
    "X.loc[X[X[column].isna()].index, column] = 0     \n",
    "\n",
    "Y = data['Price'] #certain columns\n",
    "Y = data.loc[:, ['Price']]\n",
    "Y[:5]\n",
    "\n",
    "\n",
    "# SELECT DTYPES select_dtypes Возвращает подмножество столбцов DataFrame на основе типов столбцов.\n",
    "# Находим категориальные признаки\n",
    "X.select_dtypes([np.number]).columns  #выбирает все столбцы числового формата\n",
    "integer = X_train.select_dtypes(['int', 'float']).columns.tolist()\n",
    "categorials = X_train.select_dtypes('object').columns # Находим категориальные признаки\n",
    "\n",
    "\n",
    "#astype - приведит объект pandas к указанному типу\n",
    "X[col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas observing empties\n",
    "data.isna().sum()\n",
    "data[data['height'].isna()].head()\n",
    "\n",
    "\n",
    "## Pandas и set()\n",
    "#if = 0  значит пропущены сразу оба значения\n",
    "set(data[data['latitude'].isna()].index) - set(data[data['longitude'].isna()].index)\n",
    "\n",
    "# пересечение пустого и непустого \n",
    "set(data[data['latitude'].isna()]['z_address']) & set(data[~data['latitude'].isna()]['z_address'])\n",
    "\n",
    "## Pandas delete empty rows\n",
    "data = data[~(data['height'].isna())]\n",
    "data = data[~(data['weight'].isna())]\n",
    "\n",
    "# Pandas заполнить пропуски значением \n",
    "X_train[categorials] = X_train[categorials].fillna('nan')\n",
    "[X_train[integer].fillna(-999)\n",
    "\n",
    "#Remove missing values.\n",
    "X.dropna() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369adc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy\n",
    "# [sport] - categorical value (golf, swimming, boxing, etc.)\n",
    "# adding own column for each category - with filling 0 and 1\n",
    "X = pd.get_dummies(data, columns=['sport'], drop_first=True)\n",
    "\n",
    "## когда много столбцов для трейна и теста (чекинг и объединение)\n",
    "# дамми для трейна и для теста\n",
    "dummy_train = pd.get_dummies(X_train[categorials], columns=categorials)\n",
    "dummy_test = pd.get_dummies(X_test[categorials], columns=categorials)\n",
    "\n",
    "dummy_cols = list(set(dummy_train) & set(dummy_test)) #возвращает уник столбцы \n",
    "\n",
    "#такая перезапись дамми датафреймов\n",
    "dummy_train = dummy_train[dummy_cols]\n",
    "dummy_test = dummy_test[dummy_cols]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1a09d-4bf9-4868-bce4-5c7feffd419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete column\n",
    "del X['gender']\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True) #inplace - выполните операцию на месте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas Series - just one column\n",
    "serie = pd.Series(data=le.transform(data['gender']))\n",
    "serie.head()\n",
    "\n",
    "# Series : get median\n",
    "serie.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a650f51-c94c-432a-a6ec-1e86b0b9f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "## заполенние пустот latitude = Null сравнивая по адресу аналогичными объектами где latitude != Null\n",
    "for i, el in data[data['latitude'].isna()].iterrows():\n",
    "    cur_el = data[data['z_address'] == el['z_address']]\n",
    "    if cur_el.shape[0] > 0:\n",
    "        data.loc[i, 'latitude'] = cur_el.iloc[0]['latitude']\n",
    "        data.loc[i, 'longitude'] = cur_el.iloc[0]['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777541d3-27bd-421e-9b5c-5c11ad2be043",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_24552/1700963579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Pandas datetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#amount minutes from 1970\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Pandas datetime \n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#amount minutes from 1970\n",
    "data['date'] = [t.timestamp() for t in data['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5479a",
   "metadata": {},
   "source": [
    "# 3. Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cabc48f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00110855 -0.28954407]\n"
     ]
    }
   ],
   "source": [
    "#1. Return evenly spaced values within a given interval.\n",
    "np.arange(-3, 3, step=0.5, dtype=int)\n",
    "np.arange(x_min, x_max, h).shape\n",
    "\n",
    "\n",
    "#2. Return evenly spaced numbers over a specified interval.\n",
    "np.linspace(-1, 2.5, num=10)\n",
    "\n",
    "# Повторное заполнение экземпляра Singleton RandomState.\n",
    "np.random.seed(9)\n",
    "params = np.random.normal(loc=0.0, scale=1.0, size=(2,))  #loc ~ mean; scale ~ std\n",
    "print(params)\n",
    "\n",
    "\n",
    "#3.1. Random numbrs from 0 to 90 , 1000 examples\n",
    "age_owner = np.random.choice(90, n_samples) + 21\n",
    "\n",
    "\n",
    "#3.2. Случайные INT (например для случаного предсказания) \n",
    "clusters_random = np.random.randint(low=0, high=2, size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Uniform distribution and normal (Gaussian) distribution\n",
    "np.random.uniform(low=0, high=100, size=N)\n",
    "np.random.normal(scale=5, size=N)\n",
    "\n",
    "# ex 1 , create dataset 2x + 1 + e\n",
    "N = 100\n",
    "X = np.random.uniform(low=0, high=100, size=N)\n",
    "Y = 2*X + 1 + np.random.normal(scale=5, size=N)\n",
    "\n",
    "# ex 2, random square array\n",
    "params = np.random.normal(size=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, stop, num=50\n",
    "np.linspace(0, 1, 5) #-> [0.  , 0.25, 0.5 , 0.75, 1.]\n",
    "\n",
    "#возвращает 2D декартовы координаты на основе координат, содержавшихся \n",
    "#в векторах x и y X матрица, где каждая строка является копией x , и Y матрица, где каждый столбец является копией y\n",
    "# используется для создания прямоугольной сетки из двух заданных одномерных массивов\n",
    "np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "#Возвращает непрерывный сглаженный массив.[1, 4] [2, 5] -> [1, 4, 2, 5]\n",
    "xx.ravel()\n",
    "\n",
    "\n",
    "#Переводит кусковые объекты в конкатенацию по второй оси.\n",
    "np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "#-> array([[1, 4],\n",
    "#       [2, 5],\n",
    "#       [3, 6]])\n",
    "\n",
    "\n",
    "#Переводит отрезные объекты в конкатенацию по первой оси.\n",
    "np.r_['1,2,0', [1,2,3], [4,5,6]]\n",
    "#array([[1, 4],\n",
    "#       [2, 5],\n",
    "#       [3, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9fad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return an array of zeros with the same shape and type as a given array.\n",
    "np.zeros_like([0., 1., 2.], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef0939-5572-476a-a974-f3d6e6f84d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate , reshape\n",
    "np.concatenate((x, y.reshape(len(y), 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1687b37b-b80e-4454-9d62-5e9bd1163286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -4,  1,  1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассчитать n-ю дискретную разность вдоль данной оси.\n",
    "k_inertia = [7, 6, 5, 1, 2, 3]\n",
    "diff = np.diff(k_inertia)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9529d9da-4416-420e-af7f-87ab5580b2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возвращает индексы минимальных значений по оси.\n",
    "np.argmin(np.array([1, 2, 0, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36715960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze class imbalance in the targets\n",
    "counts = np.bincount(train_targets[:, 0])\n",
    "counts\n",
    "\n",
    "print(\n",
    "    f\"Number of positive samples in training data: {counts[1]} ({100 * float(counts[1]) / len(train_targets):.2f}% of total)\"\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65f22c-521a-4af6-b341-acc261fcd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True, если два массива по элементам равны в пределах допуска.\n",
    "np.allclose(prediction.sum(-1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff8f2fe",
   "metadata": {},
   "source": [
    "# 4. seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36faf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seaborn graphics\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# 1. зависимости между конкретными переменными  (также смотри Корреляция в pandas .corr() )\n",
    "sns.pairplot(data=df, hue='HeartDisease', diag_kind=\"kde\")\n",
    "\n",
    "# 2. Getting a distibution graph\n",
    "sns.displot(\n",
    "    data=df, x=\"Age\", hue=\"Survived\", col=\"Sex\",\n",
    "    kind=\"hist\", height=5, aspect=.6,\n",
    ")\n",
    "\n",
    "# 3. histograms \n",
    "sns.histplot(data=penguins, x=\"MedHouseVal\")\n",
    "\n",
    "# 4. boxplot или график с усами\n",
    "sns.boxplot(x=df['MedHouseVal'])\n",
    "\n",
    "# 5. FacetGrid построение в сетке\n",
    "FacetGrid = sns.FacetGrid(data=df, col=\"Embarked\", height=5, aspect=1.6)\n",
    "FacetGrid.map_dataframe(sns.pointplot, x='Pclass', y='Survived', hue='Sex', order=None )\n",
    "FacetGrid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49adc5a",
   "metadata": {},
   "source": [
    "# 5. matplotlib \n",
    "\n",
    "- plot - draw by points X and Y. Points are connected according to an order in arrays.\n",
    "\n",
    "- scatter - for dots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.\n",
    "plt.figure(figsize=(12,9)) # set size\n",
    "plt.plot(Y.index, Y, 'o') # add points as round dots\n",
    "plt.plot(Y.index, y_predict, 'y^')  # add points as rectangles dots\n",
    "# 'o' - dots\n",
    "# 'y^' - rectangles\n",
    "# 'k-' 0 line\n",
    "plt.show() # draw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.\n",
    "plt.figure(figsize=(10, 7)) # set size\n",
    "plt.scatter(x, y, s=50) # add points\n",
    "plt.plot(x_, y_poly_pred, color='m') # add connected points as line\n",
    "plt.show() # draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de285e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. for classification ????\n",
    "x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
    "y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
    "\n",
    "h = .02\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(8,8))\n",
    "plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Рисование датасета с разделяющей гиперплосколстью (используется decision_function)\n",
    "\n",
    "# Author: Virgile Fritsch <virgile.fritsch@inria.fr>\n",
    "# License: BSD 3 clause\n",
    "classifiers = {\n",
    "    \"OCSVM\": OneClassSVM(nu=0.1), # nu - % выбросов\n",
    "    \"IsoForest\": IsolationForest(),\n",
    "    \"ELL\": EllipticEnvelope(contamination=0.2),\n",
    "    \"LOF\": LocalOutlierFactor(novelty=True)\n",
    "}\n",
    "colors = ['m', 'g', 'b', 'y']\n",
    "legend1 = {}\n",
    "legend2 = {}\n",
    "\n",
    "\n",
    "# Выделяем границы обнаружения выбросов с помощью нескольких классификаторов\n",
    "# метод meshgrid используется для красивой отрисовки\n",
    "xx1, yy1 = np.meshgrid(np.linspace(0, 6, 500), np.linspace(1, 4.5, 500))\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    plt.figure(1, figsize=(12,9))\n",
    "    clf.fit(X1)\n",
    "    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "    Z1 = Z1.reshape(xx1.shape)\n",
    "    legend1[clf_name] = plt.contour(\n",
    "        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n",
    "    \n",
    "    plt.contourf(xx1, yy1, Z1, cmap=plt.cm.Blues_r) # создание контуров    \n",
    "    \n",
    "legend1_values_list = list(legend1.values())\n",
    "legend1_keys_list = list(legend1.keys())\n",
    "\n",
    "# Отрисовка результатов (= форма облака точек данных)\n",
    "plt.figure(1, figsize=(12,9))  # 2 кластера\n",
    "plt.title(\"Обнаружение выбросов в наборе данных (вина)\")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='black')\n",
    "\n",
    "plt.xlim((xx1.min(), xx1.max()))\n",
    "plt.ylim((yy1.min(), yy1.max()))\n",
    "plt.legend((legend1_values_list[0].collections[0], legend1_values_list[1].collections[0], legend1_values_list[2].collections[0], legend1_values_list[3].collections[0],),\n",
    "           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2], legend1_keys_list[3],),\n",
    "           loc=\"upper center\", \n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.ylabel(\"ash\")\n",
    "plt.xlabel(\"malic_acid\")\n",
    "plt.show()\n",
    "print('Получаем разделяющую поверхность, где видны выбросы данных')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28751a",
   "metadata": {},
   "source": [
    "# 6. preprocessing (PolynomialFeatures, LabelEncoder, Normalizing, imblearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.1.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# sklearn generates additional features for polynomial regression\n",
    "polynomial_features = PolynomialFeatures(degree=2,  # degree \n",
    "                include_bias=False,    # zero coef\n",
    "                interaction_only=True  # only interaction (x*y without x**2) True of False\n",
    ")\n",
    "x_poly = polynomial_features.fit_transform(x) # get new features\n",
    "\n",
    "# ultimately x_poly uses in LinearRegression()\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.2. LabelEncoder - translate string to number  ['male', 'female'] -> [0, 1]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(data['gender']) # code of values\n",
    "\n",
    "le.classes_  # -> ['female', 'male']\n",
    "\n",
    "le.transform(['male', 'female', 'male']) # -> array([1, 0, 1])\n",
    "\n",
    "##inverse_transform\n",
    "le.inverse_transform([1, 0, 1]) # -> ['male', 'female', 'male']\n",
    "##inverse_transform\n",
    "preds_cat = le.inverse_transform(preds)\n",
    "data.loc[data[data['usecode'].isna()].index, 'usecode'] = preds_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f95f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.3. saw p.6 make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler # Z - Standartization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "## Normalize the data (manual)\n",
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std\n",
    "\n",
    "\n",
    "\n",
    "## Picture Normalization\n",
    "# transform from [0,255] to [0,1]\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9c6b6-3f4c-4c8a-bd4d-148617669c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.4. CountEncoder\n",
    "#pip install category_encoders\n",
    "from category_encoders.count import CountEncoder\n",
    "ce = CountEncoder(handle_unknown=-1) #handle_unknown - как обрабатывать неизвестные метки\n",
    "ce.fit(X[cols])\n",
    "X[cols] = ce.transform(X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859e635-5265-4afd-a87e-b33f0f80e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.5 imblearn oversampling and undersampling\n",
    "\n",
    "# - Undersampling, using TomekLink\n",
    "from imblearn.under_sampling import TomekLinks, NearMiss, ClusterCentroids\n",
    "\n",
    "# -parameter \"sampling_strategy\" can be str or list of classes for resampling\n",
    "# - the same logic for 1) NearMiss(sampling_strategy='not minority', n_neighbors=2), 2) ClusterCentroids(sampling_strategy='majority')\n",
    "tl = TomekLinks(sampling_strategy=[1, 2, 0]) \n",
    "X_tl, y_tl = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "##----------------------------------\n",
    "\n",
    "# - Oversampling, using SMOTE\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "# -https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "smote = SMOTE(sampling_strategy={4: 10, 7: 10, 3: 10, 6: 18},k_neighbors=1)\n",
    "X_sm, y_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9b9b9-2fe0-4510-af0a-2098da0b3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DictVectorizer преобразует kist of features в векторы.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# мы только рассмотрим топ-1к наиболее частых компаний, чтобы минимизировать использование памяти\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bde35f",
   "metadata": {},
   "source": [
    "# 7. extra sklearn - (model_selection, pipeline, tree, datasets)\n",
    "- pipeline, make_pipeline \n",
    "- train_test_split, ShuffleSplit, GridSearchCV\n",
    "- plot_tree\n",
    "- sklearn.datasets (make_classification)\n",
    "- cross_val_score\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033968e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Pipeline - contains with several sklearn objects\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. split dataset to Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05957d55-8bda-4889-a479-c216072a47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=5, random_state=1, test_size=0.25, train_size=None)\n",
      "Fold 0:\n",
      "  Train: index=[4 0 3 5]\n",
      "  Test:  index=[2 1]\n",
      "Fold 1:\n",
      "  Train: index=[0 5 3 1]\n",
      "  Test:  index=[4 2]\n",
      "Fold 2:\n",
      "  Train: index=[5 3 4 1]\n",
      "  Test:  index=[2 0]\n",
      "Fold 3:\n",
      "  Train: index=[4 0 2 1]\n",
      "  Test:  index=[3 5]\n",
      "Fold 4:\n",
      "  Train: index=[0 3 4 2]\n",
      "  Test:  index=[1 5]\n"
     ]
    }
   ],
   "source": [
    "### 3. НЕ ПОНИМАЮ . Бутстреп-семплирование?  Перекрестный валидатор случайных перестановок\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 1]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2])\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=1)\n",
    "rs.get_n_splits(X)\n",
    "\n",
    "print(rs)\n",
    "for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9088ecd5-4348-40aa-b407-5e0cb7927fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. GridSearchCV Полный поиск по заданным значениям параметров для оценщика. GridSearch просто перебирает все параметры. Он, конечно, найдет их самое оптимальное значение, но вопрос во времени.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# init data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# init params\n",
    "c_range = list(range(1, 31))\n",
    "parameters = dict(kernel=['linear', 'rbf'], C=c_range)\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':c_range}\n",
    "print(parameters)\n",
    "\n",
    "# models\n",
    "svc = svm.SVC()\n",
    "\n",
    "# init grid \n",
    "clf = GridSearchCV(svc, parameters, cv=10, scoring='accuracy') # cv - cross validation\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# view the complete results (list of named tuples)\n",
    "for k in grid.cv_results_:\n",
    "    print(k, \":\", grid.cv_results_[k][0])\n",
    "    break\n",
    "    \n",
    "    \n",
    "# создать list с усредненными скорами\n",
    "test_scores = grid.cv_results_['mean_test_score']\n",
    "print(test_scores)\n",
    "\n",
    "\n",
    "# график усредненных скоров\n",
    "plt.plot(k_range, test_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# Лучший скор\n",
    "print(grid.best_score_)\n",
    "\n",
    "# dict в котором хранятся лучшие параметры\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Объекты лучших моделей\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188cc50-eea1-4675-ba93-0780e9299738",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. RandomizedSearch - выбирает случайные значения из параметров и пробует запускать обучение с ними. Можно попробовать \"пристреляться\" к оптимальным значениям, а затем запустить GridSearch для уточнения\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "models=[\n",
    "      {'name':'NB',\"model\":BernoulliNB(), 'params':{'alpha': uniform(loc=0, scale=4)}},  \n",
    "      {'name':'Lr',\"model\": LogisticRegression()  , 'params':{'C':[0.1,0.2,0.3,0.5,0.7,1], 'penalty':['l1', 'l2']}},\n",
    "      {'name':'R',\"model\": Ridge(), 'params':{'alpha': uniform(loc=0, scale=4), 'solver':['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}},\n",
    "      {'name':'SVC',\"model\": SVC(), 'params':{'kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'gamma':['scale', 'auto']}},\n",
    "      {'name':'RF',\"model\": RandomForestClassifier(), 'params':{'n_estimators':[10,25,50,100,150,200], 'criterion':['gini', 'entropy'], 'max_depth':[3,5,7,9,11]}},\n",
    "      {'name':'KN',\"model\": KNeighborsClassifier(), 'params':{'n_neighbors':list(range(1,30)),'weights': ['uniform', 'distance'], 'p':[1,2,3]}},\n",
    "      {'name':'DT',\"model\": DecisionTreeClassifier(), 'params':{'criterion':['gini', 'entropy'], 'max_depth':[3,5,7,9,11]}}\n",
    "\n",
    "]\n",
    "\n",
    "res=[]\n",
    "for v in  models:\n",
    "    res.append((v['name'], RandomizedSearchCV(v['model'], v['params'], cv=10).fit(X_train, y_train)))\n",
    "    \n",
    "for r in res:\n",
    "    print(r[0], r[1].best_score_, r[1].best_params_)\n",
    "    \n",
    "\n",
    "y_pred = res[5][1].best_estimator_.predict(X_valid)\n",
    "wrong = y_pred != y_valid\n",
    "\n",
    "X_wrong, y_wrong, y_true_wrong = X_valid[wrong], y_pred[wrong], y_valid[wrong] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9016c-ca2f-4181-b52a-addc250f2902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d8d05-50fc-42ff-ab67-e157c62f0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Вычисление матрицы путаницы (true positive, false negative etc) для оценки точности классификации.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = grid.best_estimator_.predict(X)\n",
    "c_m = confusion_matrix(y, pred)\n",
    "c_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69077e-beed-48d0-9b70-3dd6581110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Оценка score путем перекрестной проверки.\n",
    "from sklearn.model_selection import cross_val_score # кросс валидация\n",
    "\n",
    "cross_val_score(LogisticRegression(), x, y, scoring='accuracy').mean()\n",
    "#или\n",
    "for i in list(range(1,30)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring=\"accuracy\")\n",
    "    print(i, ':', scores.mean(),  scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c284fbc-abd5-46a2-940f-48938b699404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 8 cross_validate перекрестная проверка и время достижения/оценки.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate # кросс валидация\n",
    "\n",
    "\n",
    "metric = ['accuracy','recall','precision','f1']\n",
    "scores = cross_validate(LogisticRegression(), X_train, y_train, cv=10, scoring=metric)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943fadcf-266f-4881-9fdf-a1835cf5e25a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_14896/1829999847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplot_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "### 9. свойство одиночных деревьев - возможность визуализировать их алгоритм работы\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(dt, feature_names=Xcut.columns, filled=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1af9ba-a491-4a7a-a372-a4b73246c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10. Получить сгенерированные данные\n",
    "from sklearn.datasets import make_classification\n",
    "x, y = make_classification(scale=1)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2fd33-b802-4f20-91a0-be8fd84e6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11. SKlearn datasets\n",
    "\n",
    "# луны\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', s=60)\n",
    "\n",
    "\n",
    "# ирисы\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "#цифры\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data['data'], data['target']\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "for i in range(0, 4):\n",
    "    for j in range(0,4):\n",
    "        img = np.random.choice(len(X))\n",
    "        plt.subplot(4, 4, i * 4 + j + 1)\n",
    "        plt.imshow(X[img].reshape(8, 8))\n",
    "        plt.title(y[img])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "X, y = fetch_20newsgroups_vectorized(subset='all', return_X_y=True)\n",
    "X = X[:n_samples]\n",
    "y = y[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac3fc7",
   "metadata": {},
   "source": [
    "# 8. SKLearn ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. libraries REGRESIION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge # L2 regularization\n",
    "from sklearn.linear_model import Lasso # L1 regularization\n",
    "m_r = Ridge(alpha=0.1)  # alpha is lambda , где lambda - это коэф. регуляризации\n",
    "m_l = Lasso(alpha=0.2)\n",
    "\n",
    "\n",
    "## 2. LogisticRegression (logistic CLASSIFICATION)\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "##params of LogisticRegression\n",
    "# penalty - штраф модели l1, l2, elasticnet (l1+l2), None\n",
    "# C = 1/lambda , где lambda - это коэф. регуляризации (чем С больше, тем у модели больше свободы)\n",
    "# solver - оптимизатор\n",
    "\n",
    "model1 = LogisticRegression(penalty='l1',\n",
    "                            C=0.01,\n",
    "                            solver='liblinear')\n",
    "\n",
    "model1.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model1.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "### 3. Support Vector Machine\n",
    "from sklearn.svm import SVC  # Support Vector Classification.\n",
    "from sklearn.svm import SVR  # Support Vector Regression\n",
    "\n",
    "## SVC\n",
    "SVC(kernel='poly') #kernel = {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a74cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic using\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1eb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## additional using\n",
    "model.coef_ # model's coefficients: b1, b2\n",
    "model.intercept_ # model's intercept_:b0\n",
    "\n",
    "X.iloc[0].values * model.coef_ + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d08b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0979261-e9a7-4841-863a-8dd033a23e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Деревья решений \n",
    "\n",
    "#DecisionTree  Classifier + Regressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(Xcut, y)\n",
    "clf.predict(Xcut)\n",
    "clf.predict_proba(Xcut)[:10]\n",
    "\n",
    "#Оценка важности фичей\n",
    "plt.barh(np.arange(len(clf.feature_importances_)), clf.feature_importances_)\n",
    "plt.yticks(np.arange(len(X.columns)), X.columns)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# DecisionTreeRegressor  Regressor \n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bf02f-1db9-433d-847c-d4af443a8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. KNN algorithms \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca6605-cce6-413c-bd52-2ec582a79507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e38c35-dc45-4dcf-8d48-5d27fad58899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc25440f",
   "metadata": {},
   "source": [
    "## 8.1. SKLearn scores and Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d10181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, mean_absolute_error\n",
    "\n",
    "## 1. just score\n",
    "model.score(X_train, y_train)\n",
    "\n",
    "## 2. recall\n",
    "recall_score(Y_test, pred)\n",
    "\n",
    "## 3. precision\n",
    "precision_score(Y_test, pred)\n",
    "\n",
    "# MAE\n",
    "mean_absolute_error(pred_values, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88034304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. ROC-AUC кривая (Receiver Operating Characteristic)\n",
    "from sklearn.metrics import roc_curve #счиатет fpr, tpr для разных thresholds\n",
    "\n",
    "\n",
    "fpr, tpr, thres = roc_curve(Y_test, predictions[:, 1])\n",
    "for line in zip(fpr[:10], tpr[:10], thres[:10]):\n",
    "    print(line)\n",
    "    \n",
    "    \n",
    "## 5. Area under ROC AUC\n",
    "# считает площадь под графиком\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "roc_auc_score(Y_test, predictions[:, 1])\n",
    "\n",
    "#6. auc\n",
    "#Про разницу между AUC И ROC тут https://stackoverflow.com/questions/31159157/different-result-with-roc-auc-score-and-auc\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc71c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. PR AUC\n",
    "# метод для построения графика precision-recall PR AUC\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "pr, re, thres = precision_recall_curve(Y_test, predictions[:, 1])\n",
    "\n",
    "for line in zip(re[:10], pr[:10], thres[:10]):\n",
    "    print(line)\n",
    "    \n",
    "plt.plot(re, pr)\n",
    "plt.plot(thres, pr[:1666])\n",
    "plt.plot(thres, re[:1666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Все метрики классификации classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.array(y_test), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee82ab-8c95-4804-8814-002e77228bfb",
   "metadata": {},
   "source": [
    "# 9. SKLearn Ansambles: Бэггинг, Стекинг и Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b70140-8afa-4461-a8ca-466b520d2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Бэггинг (один алгоритм - разные семплы из датасета)\n",
    "### 1.1. RandomForestRegressor - а-ля специальный Бэггинг для деревьев решений\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestRegressor(random_state=10)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "clf_rf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "#OOB-score позволяет не делать отдельный validation dataset, но обычно приводит к недооценке реального качества\n",
    "print(clf_rf.oob_score_)\n",
    "\n",
    "# наиболее важные признаки\n",
    "imp = pd.Series(clf_rf.feature_importances_) # use best_estimator_ for GridSearchCV\n",
    "\n",
    "# imp выводим на график\n",
    "imp.index = pd.Series(X_train.columns)\n",
    "imp = imp.sort_values(ascending=False)\n",
    "plt.title('Top 5 feature importances')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Gini')\n",
    "plt.bar(imp.head().index, imp.head())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f1787-6077-499a-ba35-db86c54605f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2. Классификатор для экстра-деревьев.\n",
    "# Этот класс реализует мета-оценку, которая подходит для ряда рандомизированных деревьев решений на различных подвыборках \n",
    "# и использует усреднение для улучшения прогнозной точности и контроля над примеркой.\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y) # обучение\n",
    "\n",
    "# важность признаков (считай фича селекшн)\n",
    "for i in range(len(names)-1):\n",
    "    print(f\"{names[i]}\\t- {model.feature_importances_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92062e0-d811-4416-8ccd-17164b2c42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.3. Бэггинг от sklearn (! на ОДНОМ адгоритме)\n",
    "#подбирает каждый базовый регрессор к случайным подмножествам исходного набора данных, а затем объединяет их отдельные прогнозы (путем голосования или путем усреднения) для формирования окончательного прогноза\n",
    "from sklearn.ensemble import  BaggingRegressor\n",
    "\n",
    "# Алгоритм для беггинга\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "bagging = BaggingRegressor(KNeighborsRegressor(),\n",
    "                           n_estimators=10,\n",
    "                           max_samples=0.5,\n",
    "                           max_features=0.5,\n",
    "                           random_state=10)\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545be71-0df2-40e6-8679-86f5648d0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. Стекинг (много алгоритмов)\n",
    "#  SKlearn Стекинг: Regressor + Classifier\n",
    "from sklearn.ensemble import StackingRegressor, StackingClassifier\n",
    "\n",
    "# some models for ensemble StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor # KNN\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import RidgeCV  # with L1 regularization?\n",
    "\n",
    "\n",
    "# Создаем стэккинг и обучаем его на наших данных \n",
    "Regressor = StackingRegressor(\n",
    "    [\n",
    "        ('LinearRegression', LinearRegression()),\n",
    "        ('KNeighborsRegressor', KNeighborsRegressor()),\n",
    "        ('DecisionTree', DecisionTreeRegressor())\n",
    "    ], RidgeCV())\n",
    "\n",
    "Regressor.fit(X_train, y_train)\n",
    "Regressor.score(X_train, y_train)\n",
    "\n",
    "# метрици качества на каждом отдельном алгоритме\n",
    "for i in Regressor.named_estimators:\n",
    "    print(f'Score on train  with model {i} {Regressor.named_estimators_[i].score(X_train, y_train)}')\n",
    "    print(f'Score on test  with model {i} {Regressor.named_estimators_[i].score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498fb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94517e45-1b1e-4cd9-9578-30d86841d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Бустинг Boosting - последовательное обучение компенсируя ошибки предыдущего шага\n",
    "\n",
    "# 3.1. SKleaern градиентный бустинг (медленный)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "boosting.fit(X_train, y_train)\n",
    "boosting.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835ddbb-098f-4a16-a672-db2e55520739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2. LGBMRegressor из пакета lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=10)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029fa7e-a1b7-4f76-b280-f1d428770878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(random_state=10)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5ae51-a2e1-4d89-8d14-07b0c7df07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. catboost от яндекса\n",
    "#!pip install catboost\n",
    "from catboost import CatBoostRegressor \n",
    "\n",
    "cat = CatBoostRegressor(random_state=10)\n",
    "cat.fit(X_train, y_train)\n",
    "cat.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640614d",
   "metadata": {},
   "source": [
    "# 10. SKLearn Outliers Detecting (Emissions): OneClassSVM, IsolationForest и DBSCAN etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e658b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. some sklearn methods for DETECTING OUTLINES\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "search_emissions = [\n",
    "            OneClassSVM(nu=0.01), \n",
    "            IsolationForest(), \n",
    "            EllipticEnvelope(contamination=0.03), \n",
    "            LocalOutlierFactor(novelty=True)\n",
    "]\n",
    "\n",
    "\n",
    "for model in search_emissions:\n",
    "        print('\\n ---- ' + str(model))\n",
    "        df['emission'] = model.fit(df).predict(df)\n",
    "        \n",
    "        #fun def_score see case 16.Функия обучающая модель\n",
    "        get_score(df[df.emission == 1].drop(columns=['Type']), df[df.emission == 1].Type)\n",
    "        \n",
    "        \n",
    "# 1.2. Отрисовку границ ищи в пункте  5 п.4      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. DBSCAN Algorith clusteriaztion\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.5, min_samples=10).fit(X)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool) # маска False\n",
    "core_samples_mask[db.core_sample_indices_] = True  # В Маске проставляется True\n",
    "\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. DBSCAN Изобажение DBSCAN класетров на графике\n",
    "\n",
    "# Количество кластеров в метках без учета шума\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Оценочное кол-во кластеров: %d' % n_clusters_)\n",
    "print('Оценочное кол-во точек шума: %d' % n_noise_)\n",
    "\n",
    "# Отрисовка результата\n",
    "plt.figure(figsize=(12,9))\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "          \n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Черный цвет используется для выявления шума\n",
    "        col = [0, 0, 0, 1]\n",
    "    \n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    \n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Оценочное кол-во кластеров: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd74a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f9b759-e384-4732-a4e4-83f623724d00",
   "metadata": {},
   "source": [
    "# 11. Feature Selection и методы Уменьшения размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f37e23-66fe-47ef-8a73-e5d7f016ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  1. PCA, LDA, NCA с изображением на графиках\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# NCA\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=1) # n_components - Количество компонентов для уменьшения размерности\n",
    "pca2 = PCA(n_components=2)\n",
    "\n",
    "lda1 = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda2 = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "nca1 = NeighborhoodComponentsAnalysis(n_components=1)\n",
    "nca2 = NeighborhoodComponentsAnalysis(n_components=2)\n",
    "\n",
    "dim_reduction_methods = [('PCA', pca1, pca2), ('LDA', lda1, lda2), ('NCA', nca1, nca2)]\n",
    "\n",
    "figure, axa = plt.subplots(3,2, figsize=(15,15))\n",
    "    \n",
    "for i, (name, model1, model2) in enumerate(dim_reduction_methods):\n",
    "    \n",
    "    model1.fit(X_train, y_train)\n",
    "    model2.fit(X_train, y_train)\n",
    "\n",
    "    X_tr1 = model1.transform(X)\n",
    "    X_tr2 = model2.transform(X)\n",
    "\n",
    "    axa[i,0].scatter(X_tr2[:, 0], X_tr2[:, 1], c=y, s=30, cmap='Set1')\n",
    "    sns.histplot(x=list(X_tr1.reshape(1,-1)[0]), hue=y, ax=axa[i,1], element=\"poly\")\n",
    "    axa[i,0].set_title(f\"{name}\")\n",
    "    axa[i,0].grid()\n",
    "    axa[i,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f921c0-817a-4f5d-be7a-20725d8d0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Последовательный отбор признаков\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "sfs.get_support() #Получить маску выделенных фичей.\n",
    "\n",
    "sfs.transform(X).shape  # transform = Уменьшить Х до выбранных фичей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79ab93-da99-4cd1-9131-0bbb929977d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RFE - работает на внутренних мехаизмах модели: для деревянных алгоритмов - фича импотанс; для линейных - на коэффициентах.\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR # Support Vector Regression\n",
    "\n",
    "estimator = SVR(kernel=\"linear\") \n",
    "selector = RFE(estimator, n_features_to_select=3, step=1) # n_features_to_select - Количество фичей  для выбора\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "selector.support_  # The mask of selected features. \n",
    "\n",
    "selector.ranking_  # The feature ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58902ec0-4543-42ce-96fd-490f5753f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. важность признаков с точки зрения методов L1-регуляризации\n",
    "from sklearn.linear_model import  Lasso  # линейная модель с L1-регуляризацией\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_sc = StandardScaler().fit_transform(X) # преобразование данных\n",
    "lg_l,pred_l = [], []\n",
    "\n",
    "list_l=list(2**np.linspace(-10,10,100))\n",
    "\n",
    "# строим n-ое кол-во моделей Лассо, меняя коэффициент регуляризации, сохраняя модель и коэффициенты\n",
    "for i in range(len(list_l)):\n",
    "    m_l = Lasso(alpha=list_l[i]).fit(X_sc, Y)\n",
    "    lg_l.append(m_l)\n",
    "    pred_l.append(m_l.coef_)\n",
    "\n",
    "# рисуем отмасштабированные признаки на одном графике\n",
    "plt.figure(figsize=(12,9))\n",
    "x_l = np.linspace(0,len(pred_l),len(pred_l))\n",
    "for i in np.vstack(pred_l).T:\n",
    "    plt.plot(x_l,np.sign(i)*np.abs(i)) \n",
    "plt.ylim(-0.05,0.2)  \n",
    "plt.legend(names)      \n",
    "plt.grid()\n",
    "\n",
    "#Из графика можно видеть как сначала зануляются менее важные признаки, при малых значениях коэффициента, \n",
    "#а затем по мере увеличения коэффициента регуляризации все остальные. \n",
    "# Таким образом, можно делать выводы о важности тех или иных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe656d8-595f-4827-b2ac-9ec015205404",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Селектор признаков, который удаляет все функции низкой дисперсии.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    "\n",
    "# threshold  признаки с отклонением от тренировочного набора ниже этого порога будут удалены. \n",
    "# По умолчанию все функции сохраняются с ненулевой дисперсией, т.е. удаляются функции, имеющие одинаковое значение во всех выборках.\n",
    "selector = VarianceThreshold(threshold=1) \n",
    "selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c834f-8049-41b2-8c73-4045f3946494",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. SelectKBest - выбор фич в соответствии с К наивысших метрик.\n",
    "###    chi2 статистики неотрицательные признаки для задач классификации.\n",
    "###    f_classif  -  Значение ANOVA F между меткой/признаком для задач классификации.\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a1471-b57a-41b2-8613-ac2502668897",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. SelectFromModel выбор признаков на основе весов важности.\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(solver='liblinear', penalty='l1')).fit(X, y)\n",
    "\n",
    "print(selector.get_support()) # маска выбранных фичей \n",
    "X = selector.transform(X) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9772b-6cef-4ae2-9777-3bb5d7e9344d",
   "metadata": {},
   "source": [
    "## 11.2. РСА (метод главных компонент) ручная реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d51c9-30d0-4cbb-be18-5a54a21a5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. генерируем искусственные данные\n",
    "x = np.arange(1,11) * (100 + np.random.randn(10))\n",
    "y = 2 * x + np.random.randn(10)*5\n",
    "X = np.vstack((x,y))\n",
    "\n",
    "## 2.  # вычтем из переменных среднее значение и запоминаем среднее для обратного преобразования \n",
    "Xcentered = (X[0] - x.mean(), X[1] - y.mean()) # вычтем из переменных среднее значение\n",
    "m = (x.mean(), y.mean())  # запоминаем среднее для обратного преобразования \n",
    "print(\"Средние значения: \", m)\n",
    "\n",
    "## 3. Посчитаем матрицу ковариаций. Делаем это с помощью бибилиотеки numpy.\n",
    "covmat = np.cov(Xcentered)\n",
    "print(covmat, \"\\n\")\n",
    "print(\"Дисперсия X: \", np.cov(Xcentered)[0,0])\n",
    "print(\"Дисперсия Y: \", np.cov(Xcentered)[1,1])\n",
    "print(\"Ковариация X и Y: \", np.cov(Xcentered)[0,1])\n",
    "\n",
    "# 4. Далее раскладываем матрицу ковариаций на собственные вектора. \n",
    "_, vecs = np.linalg.eig(covmat) # Вычисление собственных значений и правых собственных векторов квадратного массива.\n",
    "v = vecs[:,1]\n",
    "Xnew = np.dot(v, Xcentered) # Берем один собственный вектор и с помощью этого вектора преобразуем наши центрованные данные в новые данные.\n",
    "print(Xnew) # получается вектор, который уже повернут в новом пространстве.\n",
    "\n",
    "# 5. В итоге, получаем восстановленные с достаточно хорошей точностью данные.\n",
    "n = 8     #номер элемента случайной величины\n",
    "Xrestored = np.dot(Xnew[n],v) + m\n",
    "print('Восстановленные: ', Xrestored)\n",
    "print('Original: ', X[:,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4f7ec",
   "metadata": {},
   "source": [
    "# 12. Алгоритм градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Метод градиентного спуска\n",
    "\n",
    "# 1.1. Подготовка данных \n",
    "N = 100\n",
    "X = np.random.uniform(low=0, high=100, size=N)\n",
    "Y = 2*X + 1 + np.random.normal(scale=5, size=N) # y = 2x + 1 + e\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.show()\n",
    "\n",
    "# 1.2. Параметры GD\n",
    "EPOCHS = 20             #Количество эпох\n",
    "LEARNING_RATE = 0.0001  #Скорость обучения\n",
    "\n",
    "\n",
    "# 1.3. Formula MSE\n",
    "def cost_function(X, y, theta0, theta1):\n",
    "    total_cost = 0\n",
    "    for i in range(len(X)):\n",
    "        pred = theta0 + theta1 * X[i]\n",
    "        total_cost += (pred - y[i]) ** 2\n",
    "    return total_cost / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b89eb",
   "metadata": {},
   "source": [
    "- MSE для одного объекта (N=1): $$Loss = (θ_0 + θ_1 * x - y)^2  $$ \n",
    "\n",
    "\n",
    "- Производная по нулевому параметру:\n",
    "$$\\frac{dLoss}{dθ_0} = 2 \\cdot (θ_0 + θ_1 * x - y) \\cdot 1 $$\n",
    "\n",
    "- Производная по первому параметру:\n",
    "$$\\frac{dLoss}{dθ_1} = 2 \\cdot (θ_0 + θ_1 * x - y) \\cdot x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db627197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4. d(MSE)/d(theta0)\n",
    "def der_theta0(X, y, theta0, theta1):\n",
    "    total_cost = 0\n",
    "    for i in range(len(X)):\n",
    "        pred = theta0 + theta1 * X[i]\n",
    "        total_cost += 2 * (pred - y[i])\n",
    "    return total_cost / len(X)  \n",
    "\n",
    "# 1.5. d(MSE)/d(theta1)\n",
    "def der_theta1(X, y, theta0, theta1):\n",
    "    total_cost = 0\n",
    "    for i in range(len(X)):\n",
    "        pred = theta0 + theta1 * X[i]\n",
    "        total_cost += 2 * (pred - y[i]) * X[i]\n",
    "    return total_cost / (len(X))  \n",
    "\n",
    "\n",
    "theta0 = 1  #начальное приближение для theta0\n",
    "theta1 = 1  #начальное приближение для theta1\n",
    "\n",
    "# 1.6. Алгоритм градиентного спуска\n",
    "for _ in range(EPOCHS):\n",
    "    dt0 = der_theta0(X, Y, theta0, theta1)\n",
    "    dt1 = der_theta1(X, Y, theta0, theta1)\n",
    "    \n",
    "    theta0 = theta0 - LEARNING_RATE * dt0\n",
    "    theta1 -= LEARNING_RATE * dt1\n",
    "    \n",
    "    print(\"t0:\", theta0, \"t1:\", theta1, \"cost:\", cost_function(X, Y, theta0, theta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e865",
   "metadata": {},
   "source": [
    "### Матричная запись наших градиентов\n",
    "тут w<sub>0</sub> офрмлен как константный столбец в векторе X:\n",
    "\n",
    "$$Loss = \\frac{1}{n}||Xw - Y||^{2}$$\n",
    "\n",
    "\n",
    "где используется $L_{2}$ норма:\n",
    "\n",
    "$$||X w - Y|| = \\sqrt{\\sum_{i=1}^n{(X_iw - y_i)^2}} $$\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sqrt{\\sum_{i=1}^n{(X_iw - y_i)^2}} ^{2} = \\frac{1}{n}\\sum_{i=1}^n{(X_iw - y_i)^2}$$\n",
    "\n",
    "Градиент:\n",
    "\n",
    "$$\\frac{∂ MSE}{∂ w_0} = \\frac{2}{n}\\sum{({Xw - y})}$$\n",
    "\n",
    "$$\\frac{∂ MSE}{∂ w} = \\frac{2}{n}\\sum{({Xw - y}) \\cdot X}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20  # Количество эпох\n",
    "LEARNING_RATE = 0.0001  # Скорость обучения\n",
    "\n",
    "costs = []    # массив оценок MSE\n",
    "params = []   # массив параметров\n",
    "preds = []    # массив предсказаний\n",
    "\n",
    "np.random.seed(9) # Повторное заполнение экземпляра Singleton RandomState.\n",
    "params = np.random.normal(size=(2,))\n",
    "print(params)\n",
    "\n",
    "# создание копии, т.к. иначе запишется ссылка на объект\n",
    "list_params_gd = [params.copy()]\n",
    "\n",
    "# алгоритм градиентного спуска в матричной записи\n",
    "for _ in range(EPOCHS):\n",
    "    predictions = params[0] + params[1] * X\n",
    "    preds.append(predictions)\n",
    "\n",
    "    cost = np.sum((predictions - Y) ** 2) / len(predictions)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    params[0] -= LEARNING_RATE * np.sum(predictions - Y) * (2/len(predictions))\n",
    "    params[1] -= LEARNING_RATE * np.sum((predictions - Y) * X) * (2/len(predictions))    \n",
    "\n",
    "    list_params_gd.append(params.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca53ffc",
   "metadata": {},
   "source": [
    "# 13 Алгоритм стахастического градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100            # Количество эпох\n",
    "LEARNING_RATE = 0.0001  # Скорость обучения\n",
    "\n",
    "costs = []     # массив оценок MSE\n",
    "params = []    # массив параметров\n",
    "preds = []     # массив предсказаний\n",
    "\n",
    " # Повторное заполнение экземпляра Singleton RandomState.\n",
    "np.random.seed(9)\n",
    "params = np.random.normal(size=(2,))\n",
    "print(params)\n",
    "\n",
    "# создание копии, т.к. иначе запишется ссылка на объект\n",
    "list_params_sgd = [params.copy()]\n",
    "\n",
    "\n",
    "# алгоритм градиентного спуска в матричной записи (на одном объекте, но больше итераций)\n",
    "for _ in range(EPOCHS):\n",
    "    predictions = params[0] + params[1] * X\n",
    "    preds.append(predictions)\n",
    "    \n",
    "    cost = np.sum((predictions - Y) ** 2) / len(predictions)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    i = np.random.choice(len(X))\n",
    "    current_prediction = params[0] + params[1] * X[i]\n",
    "\n",
    "    params[0] -= LEARNING_RATE * np.sum(current_prediction - Y[i]) * 2\n",
    "    params[1] -= LEARNING_RATE * np.sum((current_prediction - Y[i]) * X[i]) * 2\n",
    "\n",
    "    list_params_sgd.append(params.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675ef6d",
   "metadata": {},
   "source": [
    "# 14. Accuracy, precision, recall and ROC AUC and PR AUC\n",
    "\n",
    "<b> библиотеки смотри в 7.1. SKLearn scores</b>\n",
    "\n",
    "|                   |истинное True|истинное False|\n",
    "|-------------------|-------------|--------------|\n",
    "|предсказанное True |     tp      |      fp      |\n",
    "|предсказанное False|    fn       |      tn      |\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Подсчет ручками\n",
    "tp = 0 # True positive\n",
    "fp = 0 # False positive\n",
    "fn = 0 # False negative\n",
    "tn = 0 # True negative\n",
    "\n",
    "predictions = model.predict_proba(X_test)\n",
    "for predicted_prob, actual in zip(predictions[:, 1], Y_test):\n",
    "    if predicted_prob >= 0.5:\n",
    "        predicted = 1\n",
    "    else:\n",
    "        predicted = 0\n",
    "\n",
    "    if predicted == 1:\n",
    "        if actual == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    else:\n",
    "        if actual == 1:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870fcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d009e2c",
   "metadata": {},
   "source": [
    "# 15. L1 L2 Regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65304c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. For Logistic Regr\n",
    "#penalty - штраф модели l1, l2, elasticnet (l1+l2), None\n",
    "# C = 1/lambda , где lambda - это коэф. регуляризации (чем С больше, тем у модели больше свободы)\n",
    "# solver - оптимизатор\n",
    "model1 = LogisticRegression(penalty='l1',\n",
    "                            C=0.01,  #C=1\n",
    "                            solver='liblinear').fit(X_train, Y_train)\n",
    "predictions = model1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. For LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# for different coef of regulariztions\n",
    "lg_r, lg_l, pred_r, pred_l = [], [], [], []\n",
    "list_r = list(1.5**np.linspace(-10, 2, 10))\n",
    "list_l = list(2**np.linspace(-10, 2, 10))\n",
    "\n",
    "for i in range(len(list_r)):\n",
    "  m_r = Ridge(alpha=list_r[i]).fit(X_train, y_train)\n",
    "  m_l = Lasso(alpha=list_l[i]).fit(X_train, y_train)\n",
    "  lg_r.append(m_r)\n",
    "  pred_r.append(m_r.coef_)\n",
    "  lg_l.append(m_l)\n",
    "  pred_l.append(m_l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab0b82-b12c-4e2e-ae46-c538b3c5c3d3",
   "metadata": {},
   "source": [
    "# 16 Функция заполнения пропусков. Из занятия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a0bcd-b014-47c0-82f2-3a8056d0ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(X, column, mode='mean', value=0, columns_for_reg=None): \n",
    "    if mode == 'value':\n",
    "        X.loc[X[X[column].isna()].index ,column] = value # заполнение значением\n",
    "\n",
    "    elif mode == 'median':\n",
    "        X.loc[X[X[column].isna()].index ,column] = X[column].median()    # заполнение медианой\n",
    "    \n",
    "    elif mode == 'mode':\n",
    "        X.loc[X[X[column].isna()].index, column] = X[column].mode()   # заполнение модой\n",
    "    \n",
    "    elif mode == 'indicator': # метод индикатора (принимает значение 1 при наличие пропуска и 0 в остальных случаях)\n",
    "        X['ind_'+str(column)] = 0\n",
    "        X.loc[X[X[column].isna()].index, 'ind_'+str(column)] = 1\n",
    "        X.loc[X[X[column].isna()].index, column] = 0     \n",
    "    \n",
    "    elif mode == 'linreg':   # линейная регрессия\n",
    "        if columns_for_reg is None:\n",
    "            cols = list(X.select_dtypes([np.number]).columns) \n",
    "            cols.remove(column)\n",
    "        else:\n",
    "            cols = columns_for_reg\n",
    "        X_tmp = X.dropna()  \n",
    "        m = LinearRegression().fit(X_tmp[cols], X_tmp[column])\n",
    "        X.loc[X[X[column].isna()].index, column] = m.predict(X[X[column].isna()][cols])\n",
    "    \n",
    "    else:\n",
    "        X.loc[X[X[column].isna()].index, column] = X[column].mean()  # по умолчанию среднее значение\n",
    "    \n",
    "    return X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd73f2-ea5c-4b45-98ad-187dc9bd2b62",
   "metadata": {},
   "source": [
    "# 17. Функия обучающая модель (для оценки разных преобразований)\n",
    "\n",
    "с использованием пунка 14 для оценки разных преобразований"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef4a4e-60f6-46df-8061-9602eb90a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#здесь не совсем корректно, что будут делиться на трейн и тест только после преобразовния, \n",
    "#по идее тест должен быть сырым\n",
    "# но для изучения ОК\n",
    "def get_score(X,y, random_seed=42, model=None):\n",
    "  if model is None:\n",
    "      model = LinearRegression()\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed) \n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "  model.fit(X_train, y_train)\n",
    "  return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385152f6-5b21-4727-9cf3-da728a577551",
   "metadata": {},
   "source": [
    "# 18. Функции One-Hot, Label and Count encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e835e-37ed-45c9-bcb4-da8507249bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(X, cols):\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(X[each], prefix=each)\n",
    "        X = pd.concat([X, dummies], axis=1)\n",
    "    return X\n",
    "\n",
    "def get_label(X, cols):\n",
    "    X = X.copy()\n",
    "    for each in cols:\n",
    "        le = LabelEncoder()\n",
    "        labels = le.fit_transform(X[each])\n",
    "        X[each] = labels\n",
    "    return X\n",
    "\n",
    "def get_count(X, cols):\n",
    "    X = X.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        X[col] = X[col].astype('str')\n",
    "\n",
    "    ce = CountEncoder(handle_unknown=-1)\n",
    "    ce.fit(X[cols])\n",
    "    X[cols] = ce.transform(X[cols])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a58bc6-1844-4589-b2e6-3a158379377b",
   "metadata": {},
   "source": [
    "# 19. Работа с геоданными (DBSCAN, KMeans, reverse_geocoder, folium, keplergl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0074b9-5a99-4376-909e-63a9304703d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN пространственная кластеризация , алгоритм кластеризации данных\n",
    "#KMeans - Метод k-средних — наиболее популярный метод кластеризации\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "## БОЛЬШЕ ОПИСАНИЯ ДОБАВИТЬ\n",
    "kmeans = KMeans(n_clusters=15) # n_clusters - число кластеров\n",
    "cluster = kmeans.fit_predict(data[['latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7a3b1-1aa8-4181-ab91-e385094b48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#по широте и долготе получает доп инфу (адрес и прочее)\n",
    "import reverse_geocoder as revgc  \n",
    "\n",
    "revgc.search((data.iloc[10].latitude, data.iloc[10].longitude))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1e070-6a8f-4599-987f-3c747b806218",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_10952/658915345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     ).add_to(this_map)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplotDot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#3388FF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mthis_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# создает карты\n",
    "import folium\n",
    "\n",
    "this_map = folium.Map(prefer_canvas=True)\n",
    "\n",
    "def plotDot(point, color):\n",
    "    folium.CircleMarker(\n",
    "        location=[point.latitude, point.longitude],\n",
    "        radius=2,\n",
    "        weight=5,\n",
    "        color=color,\n",
    "        popup=point.zestimate\n",
    "    ).add_to(this_map)\n",
    "    \n",
    "data.iloc[:2000].apply(plotDot, axis=1, color='#3388FF')\n",
    "\n",
    "this_map.fit_bounds(this_map.get_bounds())\n",
    "this_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a5827-52ca-4ac3-861f-50f1a36f9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тоже создает карты\n",
    "from keplergl import KeplerGl \n",
    "\n",
    "map_ = KeplerGl(height=700)\n",
    "map_.add_data(data, 'Data')\n",
    "map_.save_to_html(file_name='./california.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966790f6-e9b3-42db-b4c5-758f93789805",
   "metadata": {},
   "source": [
    "# 20. Реализация дерева решений\n",
    "\n",
    "энтропийного критерия качества:\n",
    "\n",
    "$H(R) = -\\sum_{k=1}^{K}p_klogp_k$\n",
    "\n",
    "\n",
    "критерия Джини:\n",
    "\n",
    "$Н(R) = \\sum_{k=1}^{K}p_k(1-p_k)$\n",
    "\n",
    "\n",
    "*Information Gain (IG)* - функционал качества, отвечающий на вопрос, а сколько энтропии мы погасили при определённом разбиении? На каждом шаге разбиения при построении дерева максимизируется IG. Формула для вычисления при критерии информативности H:\n",
    "\n",
    "$IG(R) = H(R) - \\frac{|R_l|}{|R|}H(R_l) - \\frac{|R_r|}{|R|}H(R_r)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37afbbd2-3ae4-46fd-8903-9b174fd058ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 2, 2]\n",
      "#     l     IG   Hl   Hr  \n",
      "------------------------\n",
      " 0.   1   0.00 0.00 0.95\n",
      " 1.   1   0.09 0.00 0.99\n",
      " 2.   1   0.20 0.00 1.00\n",
      " 3.   1   0.35 0.00 0.97\n",
      " 4.   1   0.55 0.00 0.81\n",
      " 5.   2   0.95 0.00 0.00\n",
      " 6.   2   0.47 0.65 0.00\n",
      " 7.   2   0.20 0.86 0.00\n",
      "Деление после элемента:4\n",
      "#     l     IG   Hl   Hr  \n",
      "------------------------\n",
      " 0.   1   0.00 0.00 0.47\n",
      " 1.   1   0.04 0.00 0.49\n",
      " 2.   1   0.09 0.00 0.50\n",
      " 3.   1   0.17 0.00 0.48\n",
      " 4.   1   0.28 0.00 0.38\n",
      " 5.   2   0.47 0.00 0.00\n",
      " 6.   2   0.26 0.28 0.00\n",
      " 7.   2   0.11 0.41 0.00\n",
      "Деление после элемента:4\n"
     ]
    }
   ],
   "source": [
    "def HEntropy(l):\n",
    "    length = len(l)\n",
    "    cnt = Counter(l)\n",
    "    \n",
    "    ent = 0\n",
    "    for cl in cnt.values():\n",
    "        p = cl / length\n",
    "        l2 = np.log2(p)\n",
    "        it = -p * l2\n",
    "        ent += it\n",
    "    \n",
    "    return ent\n",
    "\n",
    "\n",
    "def HGini(l):\n",
    "    length = len(l)\n",
    "    cnt = Counter(l)\n",
    "    \n",
    "    gini = 0\n",
    "    for cl in cnt.values():\n",
    "        p_1 = cl / length\n",
    "        p_2 = (1 - p_1)\n",
    "        it = p_1 * p_2\n",
    "        gini += it\n",
    "    \n",
    "    return gini\n",
    "\n",
    "\n",
    "def IG(H, l, i):\n",
    "    left_l = l[:i]\n",
    "    right_l = l[i:]\n",
    "    return H(l) - (len(left_l) / len(l)) * H(left_l) - (len(right_l) / len(l)) * H(right_l)\n",
    "\n",
    "\n",
    "def test_H(H, l):\n",
    "    print(\"{:5} {:3}   {:4} {:4} {:4}\".format(\"#\",\"l\",\"IG\",\"Hl\",\"Hr\"))\n",
    "    print(\"-\" * 24)\n",
    "    i_max, IG_max=0, 0\n",
    "\n",
    "    for i in range(0, len(l)):\n",
    "        print(f\"{i:2}. {l[i]:3}   {IG(H, l, i):.2f} {H(l[:i]):.2f} {H(l[i:]):.2f}\")\n",
    "\n",
    "        if IG_max < IG(H, l, i):\n",
    "            i_max, IG_max = i, IG(H, l, i)\n",
    "\n",
    "    print(f'Деление после элемента:{i_max-1}')\n",
    "    \n",
    "    \n",
    "l = [1]*5 + [2]*3\n",
    "print(l)\n",
    "\n",
    "\n",
    "test_H(HEntropy, l)\n",
    "test_H(HGini, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f107e-d8ac-443e-93bb-13691c1840f2",
   "metadata": {},
   "source": [
    "# 21. Кластеры\n",
    "\n",
    "Algorithms\n",
    "- 1. KMeans\n",
    "- 2. AgglomerativeClustering\n",
    "- 3. DBSCAN\n",
    "- 4. AffinityPropagation\n",
    "\n",
    "\n",
    "Metrics:\n",
    "- adjusted_rand_score\n",
    "- silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e9777-af56-413b-97f2-3051bc62044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. четыре алгоиртма кластеризации\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, AffinityPropagation\n",
    "\n",
    "# метрики для кластеризации ARS  и Силуэт\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "\n",
    "algorithms = [\n",
    "    AffinityPropagation(),\n",
    "    KMeans(n_clusters=2),\n",
    "    AgglomerativeClustering(n_clusters=2),\n",
    "    DBSCAN(),\n",
    "]\n",
    "\n",
    "\n",
    "# Функция получения имени Алгоритма и двух score (функция-заготовка, которая считает 2 метрики качества и генерирует заголовки для графиков)\n",
    "def get_descr(algo_name, y, y_pred, X):\n",
    "    return \"{}\\nARI {:.2f}\\nSilhouette {:.2f}\".format(\n",
    "        algo_name,  \n",
    "        adjusted_rand_score(y, y_pred),\n",
    "        silhouette_score(X, y_pred)\n",
    "    )\n",
    "\n",
    "# Цикл по алгоритмам \n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    # кластеризуем и выводим картинку\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='autumn', s=60) # отображение на графике точки по осям и цвет по кластеру \n",
    "    ax.set_title(get_descr(algorithm.__class__.__name__, y, clusters, X_scaled))\n",
    "    \n",
    "    # если есть центры кластеров - выведем их\n",
    "    if algorithm.__class__.__name__ in {'KMeans', 'AffinityPropagation'}:\n",
    "        centers = algorithm.cluster_centers_\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253e91b-44cc-4784-9384-ae7abeb57ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. KMeans (подбор параметров , метод локтя)\n",
    "# KMeans минимизирует сумму квадратов расстояний объектов классов до их центроидов. В обученном классификаторе это значение находится в поле inertia_\n",
    "\n",
    "k_inertia = []\n",
    "ks = range(1,11)\n",
    "\n",
    "for k in ks:\n",
    "    clf_kmeans = KMeans(n_clusters=k)\n",
    "    clusters_kmeans = clf_kmeans.fit_predict(X_scaled, )\n",
    "    k_inertia.append(clf_kmeans.inertia_)\n",
    "    \n",
    "#Как подобрать оптимальное значение? Будем на каждом шаге смотреть изменение inertia, и в тот момент, когда оно резко замедлится (относительно предыдущего изменения) мы и остановимся\n",
    "plt.plot(ks, k_inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b5694-c36c-4828-a5de-6f826bf53f4f",
   "metadata": {},
   "source": [
    "Как подобрать оптимальное значение? Будем на каждом шаге смотреть изменение inertia, и в тот момент, когда оно резко замедлится (относительно предыдущего изменения) мы и остановимся\n",
    "\n",
    "найти оптимальное число кластеров по этой формулам:\n",
    "$${inertia_{k+1}-inertia_{k}}$$\n",
    "$$k_{opt} = argmin(\\frac{inertia_{k+1}-inertia_{k}}{inertia_{k}-inertia_{k-1}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20a126-7b28-4734-8df7-2b2f2433ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#по своей сути такой же локоть, но выглядит иначе\n",
    "diff = np.diff(k_inertia)\n",
    "plt.plot(ks[1:], diff)\n",
    "\n",
    "diff_r = diff[1:] / diff[:-1]\n",
    "plt.plot(ks[1:-1], diff_r)\n",
    "k_opt = ks[np.argmin(diff_r)+1]\n",
    "k_opt # количество кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16acf15-e998-43c9-bcca-6316fc597b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1. Дендограмма scipy\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "# Функиця для построения дендограммы (есть готовая функция для этого)\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1 \n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "# используем Агломеративную кластеризацию (Иерархическая) \n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='single') # linkage='average', 'complete', 'ward' -- Критерий увязки определяет, какое расстояние использовать между наборами наблюдений.\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### 3.2. используя linkage (агломеративная кластеризация от scipy ) & pdist (попарное расстояние между наблюдениями в n-мерном пространстве)\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "distance_mat = pdist(x)\n",
    "\n",
    "Z= hierarchy.linkage(distance_mat, method='single') #'ward'  # Выполнение иерархической/агломеративной кластеризации.\n",
    "plt.figure(figsize=(15,5))\n",
    "dn = hierarchy.dendrogram(Z,color_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273aeb02-4c03-4407-a31e-ee1ef1110c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. DBSCAN\n",
    "scan_moons = DBSCAN() # если запустить метод без указания eps то все точки в один кластер\n",
    "y_moons = scan_moons.fit_predict(X_moons)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=y_moons)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "scan_moons = DBSCAN(eps=0.1)#  указываем eps (.. И путем подбора правильного eps (eps=0.25, eps=0.5) приходим к верному кол-ву кластеров\n",
    "y_moons = scan_moons.fit_predict(X_moons)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=y_moons)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a054cf-03dc-4d5d-a10f-cda1ae9c5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1. DBSCAN Picture clusterizations \n",
    "\n",
    "# DBSCAN search centroid (there are searchig centoids for pictures) (подробнее на гитхабе ML проект 10)\n",
    "\n",
    "import cv2 # for work with picture\n",
    "import skimage # for work with picture\n",
    "from skimage.metrics import structural_similarity as ssim  # метрика для оценки картинки \n",
    "\n",
    "## Step 1. Data init\n",
    "def draw_picture(image, title, bgr=False):\n",
    "    b, g, r = cv2.split(image) # по умолчанию cv2 почему-то отдает цвета в порядке BGR вместо RGB\n",
    "    new_image = cv2.merge([r, g, b])\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(new_image)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Для загрузки картинки можно использовать код:\n",
    "img = cv2.imread('datasets_features\\palette.png')\n",
    "img = cv2.resize(img, (128, 128))\n",
    "draw_picture(img, 'original picture 13 colors (12 + white)')\n",
    "\n",
    "img = img.reshape((-1, 3))\n",
    "\n",
    "# Step 2. DBSCAN\n",
    "algorithm = DBSCAN()\n",
    "clusters = algorithm.fit_predict(img)\n",
    "\n",
    "# img = array([[255, 255, 255],\n",
    "#              [255, 255, 255],\n",
    "\n",
    "# calculate centroids\n",
    "clusters_centroids = np.concatenate((clusters.reshape((-1, 1)), img), axis=1)\n",
    "for i in set(clusters):\n",
    "    clusters_centroids[clusters_centroids[:, 0]==i, 1] = clusters_centroids[clusters_centroids[:, 0]==i, 1].mean() # Read mean\n",
    "    clusters_centroids[clusters_centroids[:, 0]==i, 2] = clusters_centroids[clusters_centroids[:, 0]==i, 2].mean() # Green mean\n",
    "    clusters_centroids[clusters_centroids[:, 0]==i, 3] = clusters_centroids[clusters_centroids[:, 0]==i, 3].mean() # Blue mean\n",
    "clusters_centroids = clusters_centroids[:, 1:]\n",
    "clusters_centroids\n",
    "\n",
    "print(f'SIMM score for eps={e}: {ssim(img, clusters_centroids, channel_axis=2, multichannel=True)}')\n",
    "\n",
    "draw_picture(clusters_centroids.astype(int).reshape((128, 128, 3)), 'DBSCAN picture 13 colors (12 + white)')\n",
    "\n",
    "\n",
    "#5.2. AgglomerativeClustering and calculate centoids for JPG\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "\n",
    "for k in [2, 5, 10, 20]:\n",
    "    algorithm = AgglomerativeClustering(n_clusters=k)\n",
    "    clusters = algorithm.fit_predict(img)\n",
    "\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(img, clusters)\n",
    "    clusters = clf.centroids_[clusters]\n",
    "    print(f'SIMM score for n_clusters={k}: {ssim(img, clusters, channel_axis=2, multichannel=True)}')\n",
    "\n",
    "draw_picture(clusters.astype(int).reshape((128, 128, 3)), 'AgglomerativeClustering picture 13 colors (12 + white)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d1b3e-3ff4-4d19-9464-957a51a69b30",
   "metadata": {},
   "source": [
    "# 22. Байесовская Оптимизация\n",
    "Значения гиперпараметров в текущей итерации выбираются с учётом результатов на предыдущем шаге. Основная идея алгоритма заключается в следующем – на каждой итерации подбора находится компромисс между исследованием регионов с самыми удачными из найденных комбинаций гиперпараметров и исследованием регионов с большой неопределённостью (где могут находиться ещё более удачные комбинации). Это позволяет во многих случаях найти лучшие значения параметров модели за меньшее количество времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d944c34-5d9c-4a86-97e9-638e5ba0a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Через hyperopt\n",
    "from hyperopt import hp, rand, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "\n",
    "#пространство поиска параметров для hyperopt\n",
    "search_space = [\n",
    "    hp.choice(label='n_neighbors', options=list(range(1,30))),\n",
    "    hp.choice(label='weights', options=['uniform', 'distance']),\n",
    "    hp.choice(label='p', options=[1,2,3,4,5]),           \n",
    "]\n",
    "\n",
    "# sample позвоялет оценить пространство параметров.\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "\n",
    "samples = []\n",
    "\n",
    "# Sample 10000 values from the range\n",
    "for _ in range(10):\n",
    "    print(sample(search_space)) # space description (3 params)\n",
    "    \n",
    "#необходимо создать функцию\n",
    "def objective(params):\n",
    "  knn = KNeighborsClassifier(n_neighbors=params[0], weights=params[1], p=params[2])\n",
    "  scores = cross_val_score(knn, X, y, cv=10, scoring=\"accuracy\")\n",
    "  return -scores.mean()\n",
    "\n",
    "\n",
    "# tpe.suggest - method Bayesian Optimisation Tree of Parzen Estimators (TPE)\n",
    "tpe_algo = tpe.suggest\n",
    "# rand.suggest - Random Search\n",
    "rand_algo = rand.suggest\n",
    "\n",
    "# Create two trials objects (Trials - объект для сохранения истории поиска)\n",
    "tpe_trials = Trials()  \n",
    "rand_trials = Trials()   # для rstate= np.random.RandomState(50)\n",
    "\n",
    "# запускаем сам процесс подбора с помощью функции fmin\n",
    "tpe_best = fmin(fn=objective, space=search_space, algo=tpe_algo, trials=tpe_trials, \n",
    "                max_evals=25)\n",
    "\n",
    "print(tpe_best)\n",
    "\n",
    "\n",
    "# Run 25 evals with the random algorithm\n",
    "rand_best = fmin(fn=objective, space=search_space, algo=rand_algo, trials=rand_trials, \n",
    "                 max_evals=25, rstate= np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66a90b2-3066-48a7-9446-6b228d9c8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Через optuna\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "import lightgbm as lgb\n",
    "\n",
    "#функция для получения score\n",
    "def scorer(model, X,y):\n",
    "    return f1_score(y, model.predict(X), average='weighted')\n",
    "\n",
    "\n",
    "#Определяем целевую функцию objective , через аргументы она будет получать специальный объект trial. С его помощью можно назначать различные гипермараметры\n",
    "def objective(trial: Trial,X,y) -> float:\n",
    "\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    param = {\n",
    "                \"n_estimators\" : trial.suggest_int('n_estimators', 400, 1000, 300),\n",
    "                'max_depth':trial.suggest_int('max_depth', 1, 11, 2),\n",
    "                'learning_rate':trial.suggest_loguniform('learning_rate',0.05,1),\n",
    "                'metric': 'multi_logloss',\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': 20,\n",
    "                #'class_weight': 'balanced',\n",
    "                'n_jobs' : -1,\n",
    "                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                'colsample_bytree': trial.suggest_uniform('feature_fraction', 0.3, 1.0),\n",
    "                'subsample': trial.suggest_uniform('bagging_fraction', 0.3, 1.0),\n",
    "            }\n",
    "   \n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    return  cross_val_score(model, X, y, scoring = scorer, cv=3).mean()\n",
    "\n",
    "#объект обучения с помощью метода optuna.create_study\n",
    "study = optuna.create_study(direction='maximize',sampler=TPESampler())\n",
    "\n",
    "#После создания объекта Study, можно приступать к оптимизации целевой функции.\n",
    "study.optimize(lambda trial : objective(trial,X_train,y_train),n_trials= 50)\n",
    "\n",
    "#специальные поля, которые позволяют посмотреть результаты после обучения\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc3288-9303-4ccc-98c8-4340eb3865db",
   "metadata": {},
   "source": [
    "# 23 Работа с текстом (CountVectorizer, Tf-Idf, word2vec, fastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac26136-1969-4cfb-abd0-bbef73c63496",
   "metadata": {},
   "source": [
    "## 23.1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad619be8-663d-41a0-b3db-01b53f73fe82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus) #обучение и трансформация\n",
    "print(vectorizer.get_feature_names_out()) # get_feature_names() for new version - имена выходных фичей для преобразования - по сути словарь.\n",
    "print(X.toarray()) # X (=sparse matrix) to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b5d811-920e-41a6-86c1-8004e934e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'and this' 'and this is' 'document' 'document is' 'document is the'\n",
      " 'first' 'first document' 'is' 'is the' 'is the first' 'is the second'\n",
      " 'is the third' 'is this' 'is this the' 'one' 'second' 'second document'\n",
      " 'the' 'the first' 'the first document' 'the second' 'the second document'\n",
      " 'the third' 'the third one' 'third' 'third one' 'this' 'this document'\n",
      " 'this document is' 'this is' 'this is the' 'this the' 'this the first']\n",
      "[[0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0]\n",
      " [0 0 0 2 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# тоже самое только с n-граммами \n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3)) \n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1562174-0f1c-498f-8145-57f03b9dc288",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 23.2. TfidfVectorizer\n",
    "\n",
    "- <b>Tfidf  (TfidfVectorizer)</b>\n",
    "<img src='https://drive.google.com/uc?export=view&id=14nvoZ71VqMfYBY6ff_7_Btx-tVwL_4lB'>\n",
    "\n",
    "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова t в пределах отдельного документа.\n",
    "\n",
    "$$tf = \\frac {n_t}{\\sum _{k}n_{k}}$$\n",
    "\n",
    "*TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)*\n",
    "\n",
    "\n",
    "\n",
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции.\n",
    "\n",
    "$$idf=\\log \\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}$$\n",
    "\n",
    "*IDF термина а = логарифм(Общее количество документов / Количество документов, в которых встречается термин а)*\n",
    "\n",
    "\n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей:\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c886ec-7b89-4438-b66f-e2967e538d50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_7812/2700788616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_processed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'address'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#->   <11330x50 sparse matrix of type '<class 'numpy.float64'>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_processed' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#max_features сколько уник слов брать\n",
    "# обучает так называемую sparse matrix - которая не хранит в себе нули (пропуски) а только ценные значения\n",
    "tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "\n",
    "vectorized = tfidf.fit_transform(data_processed['address'])\n",
    "vectorized\n",
    "#->   <11330x50 sparse matrix of type '<class 'numpy.float64'>'\n",
    "#     with 17826 stored elements in Compressed Sparse Row format>\n",
    "\n",
    "##11330x50 - 17826 = количество нудевых значений, но так как это sparse matrix - они не запоминаются \n",
    "#запомнилось только 17826 значений\n",
    "\n",
    "\n",
    "# to pandas\n",
    "vectorized_df = pd.DataFrame(vectorized.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5308b7-4987-4d1e-9d83-c1af4ef9c37e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_7812/337539511.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names()) # получить словарь на котором рассчитывались tfidf\n",
    "print(X.idf_)  # idf столько же сколько элементов в словаре\n",
    "print(X) # тут Х это sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e1ca2-35f4-4e66-ad88-415774dc4887",
   "metadata": {},
   "source": [
    "## 23.3 word2vec и fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa0dbee-1763-4365-8fca-489c82e2c0de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ECF33~1.RAD\\AppData\\Local\\Temp/ipykernel_7812/536033490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# !wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# load the google word2vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# скачивание обученной языковой модели word2vec\n",
    "# !wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aad89e4f-cd07-4581-b272-1541c8b0c5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer']\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36, 290)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "\n",
    "print(common_texts[0])\n",
    "['human', 'interface', 'computer']\n",
    "print(len(common_texts))\n",
    "\n",
    "model = FastText(vector_size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(corpus_iterable=common_texts)\n",
    "model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a47a5b-b9f5-4839-aef8-00a45df79437",
   "metadata": {},
   "source": [
    "## 23.4 Токенизация, разбор предложений, лемматизация, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45642db5-b648-46bf-8dd9-80d1cdb86d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy authentication required>\n",
      "[nltk_data] Error loading stopwords: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy authentication required>\n"
     ]
    }
   ],
   "source": [
    "import nltk # nltk используется преумещественно для предобработки # nltk - набор тулов для обработки естественного языка\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028df589-bbb3-4f3b-9734-26bff8e3af8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize - разбиение на предложения\n",
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = sent_tokenize(text) #\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73059983-c38f-4bdf-b46e-00e61a15aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________\n",
      "1.origin___________________\n",
      "Обработка текстов на естественном языке — общее направление искусственного интеллекта и математической лингвистики.\n",
      "2.only kirilic___________________\n",
      "Обработка текстов на естественном языке  общее направление искусственного интеллекта и математической лингвистики\n",
      "3.word tokenizing___________________\n",
      "['Обработка', 'текстов', 'на', 'естественном', 'языке', 'общее', 'направление', 'искусственного', 'интеллекта', 'и', 'математической', 'лингвистики']\n",
      "4.deleting stop-word___________________\n",
      "['Обработка', 'текстов', 'естественном', 'языке', 'общее', 'направление', 'искусственного', 'интеллекта', 'математической', 'лингвистики']\n",
      "5.lemmitize___________________\n",
      "['обработка', 'текст', 'естественный', 'язык', 'общий', 'направление', 'искусственный', 'интеллект', 'математический', 'лингвистика']\n",
      "__________________________\n",
      "1.origin___________________\n",
      "Оно изучает проблемы компьютерного анализа и синтеза текстов на естественных языках.\n",
      "2.only kirilic___________________\n",
      "Оно изучает проблемы компьютерного анализа и синтеза текстов на естественных языках\n",
      "3.word tokenizing___________________\n",
      "['Оно', 'изучает', 'проблемы', 'компьютерного', 'анализа', 'и', 'синтеза', 'текстов', 'на', 'естественных', 'языках']\n",
      "4.deleting stop-word___________________\n",
      "['Оно', 'изучает', 'проблемы', 'компьютерного', 'анализа', 'синтеза', 'текстов', 'естественных', 'языках']\n",
      "5.lemmitize___________________\n",
      "['оно', 'изучать', 'проблема', 'компьютерный', 'анализ', 'синтез', 'текст', 'естественный', 'язык']\n",
      "__________________________\n",
      "1.origin___________________\n",
      "Применительно к искусственному интеллекту анализ означает понимание языка, а синтез — генерацию грамотного текста.\n",
      "2.only kirilic___________________\n",
      "Применительно к искусственному интеллекту анализ означает понимание языка а синтез  генерацию грамотного текста\n",
      "3.word tokenizing___________________\n",
      "['Применительно', 'к', 'искусственному', 'интеллекту', 'анализ', 'означает', 'понимание', 'языка', 'а', 'синтез', 'генерацию', 'грамотного', 'текста']\n",
      "4.deleting stop-word___________________\n",
      "['Применительно', 'искусственному', 'интеллекту', 'анализ', 'означает', 'понимание', 'языка', 'синтез', 'генерацию', 'грамотного', 'текста']\n",
      "5.lemmitize___________________\n",
      "['применительно', 'искусственный', 'интеллект', 'анализ', 'означать', 'понимание', 'язык', 'синтез', 'генерация', 'грамотный', 'текст']\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "text = \"Обработка текстов на естественном языке — общее направление искусственного интеллекта и математической лингвистики. Оно изучает проблемы компьютерного анализа и синтеза текстов на естественных языках. Применительно к искусственному интеллекту анализ означает понимание языка, а синтез — генерацию грамотного текста.\"\n",
    "sentences = nltk.sent_tokenize(text, language=\"russian\")\n",
    "for sentence in sentences:\n",
    "    print('__________________________')\n",
    "    print('1.origin___________________')\n",
    "    print(sentence)  # original\n",
    "    \n",
    "    print('2.only kirilic___________________')\n",
    "    sentence_ = re.sub(r\"[^А-Яа-яёЁ ]\",\"\", sentence)  # оставляем только кириллицу\n",
    "    print(sentence_)\n",
    "    \n",
    "    print('3.word tokenizing___________________')\n",
    "    tokens = nltk.word_tokenize(sentence_)  # токенизация на слова\n",
    "    print(tokens)\n",
    "    \n",
    "    print('4.deleting stop-word___________________')\n",
    "    tokens = [i for i in tokens if (i not in stop_words)]  # удаление стоп слов\n",
    "    print(tokens)\n",
    "    \n",
    "    print('5.lemmitize___________________')\n",
    "    tokens = list(map(lambda x: morph.parse(x)[0].normal_form, tokens))  # лемматизация - приведение к нормальной форме\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375403cc-2913-4d30-a281-4df846cf0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбивка на токены с помощью WordPunctTokenizer - токенезатор разбивает на слова и пунктуацию\n",
    "import nltk  \n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer() # токенезатор (слова и пунктуация - примет за токены)\n",
    "\n",
    "data[\"FullDescription\"] = data[\"FullDescription\"].astype(str).apply(\n",
    "    lambda x: ' '.join(tokenizer.tokenize(x.lower())), 1)\n",
    "\n",
    "data[\"Title\"] = data[\"Title\"].astype(str).apply(\n",
    "    lambda x: ' '.join(tokenizer.tokenize(x.lower())), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb31236-5cb0-4dbf-a127-799b99ac90a0",
   "metadata": {},
   "source": [
    "## 23.5. Логистическая регрессия + TFIDF для распознание спама (тут ошибка что train test split только после TfIdFVectorizer !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af1242-80d3-4adc-b78b-d3bd8f313222",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Датасет распознавание спама: Загружаем данные и разархивируем\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip\n",
    "\n",
    "df = pd.read_table('SMSSpamCollection',sep='\\t',header=None, names=['label','sms_message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d62885-f5ce-4662-bbe6-aed9cca3b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Корпус: создание объекта list где каждый элемент это все тексты определенного класса \n",
    "data_corp = [ \" \".join(df[df['label'] == l]['sms_message'].tolist()) for l in list(df.label.unique()) ]\n",
    "\n",
    "## 2. TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() \n",
    "vectorizer.fit(data_corp) # получаем sparse matrix\n",
    "\n",
    "vectorizer.transform(['The guy did some bitching but I acted like']) # получаем sparse matrix для демонтрации \n",
    "res_tfidf = vectorizer.transform(df['sms_message'].tolist()) # переводим все тексты в tfidf пространство \n",
    "\n",
    "## 3. Подготвка даннных\n",
    "le = LabelEncoder().fit(df['label'])\n",
    "df['cat_label'] = le.transform(df['label'])\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts=train_test_split(res_tfidf, df['cat_label'], test_size=0.2)\n",
    "\n",
    "## 4. Бинарная классиикация\n",
    "lr = LogisticRegression().fit(X_tr, y_tr)\n",
    "y_pred = lr.predict(X_ts)\n",
    "f1_score(y_ts, y_pred)\n",
    "print(classification_report(y_ts, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a924008-f6d2-4fc0-87c6-b7a9561f6e63",
   "metadata": {},
   "source": [
    "## 23.6. Наивный Байесовский классификатор для распознавания спама (sklearn и ручная реализация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06c920-8f68-403e-9181-6528cca36b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#заменяем целевую переменную\n",
    "df['label'] = df.label.map({'ham':0,'spam':1})\n",
    "\n",
    "\n",
    "#Делим на трейновую и тестовую выборку\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], random_state=1)\n",
    "\n",
    "\n",
    "#Считаем количество слов в предложениях\n",
    "#получим матрицу размером (количество предложений Х размер словаря)\n",
    "count_vector = CountVectorizer()\n",
    "train_tensor = count_vector.fit_transform(X_train).toarray().astype('float')\n",
    "test_tensor = count_vector.transform(X_test).toarray().astype('float')\n",
    "\n",
    "# размер трейновой выборки: получаем (кол документов, размер словаря )\n",
    "train_tensor.shape\n",
    "\n",
    "# всего слов в предложениях\n",
    "train_tensor.sum()\n",
    "\n",
    "########################\n",
    "# тензор с предложениями класса spam\n",
    "spam_train_tensor = train_tensor[(y_train == 1).values]\n",
    "\n",
    "# тензор с предложениями класса not_spam\n",
    "not_spam_train_tensor = train_tensor[(y_train == 0).values]\n",
    "\n",
    "#Вероятность слова при условии что предложение спам \n",
    "p_w_spam = (spam_train_tensor.sum(axis=0)) / (spam_train_tensor.sum())\n",
    "\n",
    "#Вероятность слова при условии что предложение не спам\n",
    "p_w_not_spam = (not_spam_train_tensor.sum(axis=0)) / (not_spam_train_tensor.sum())\n",
    "\n",
    "\n",
    "# вероятность, что любое сообщение спам (кол спама делим на общее)\n",
    "p_spam = (y_train == 1).values.sum() / len(y_train)\n",
    "\n",
    "# вероятность, что любое сообщение не спам (кол не спама делим на общее)\n",
    "p_not_spam = (y_train == 0).values.sum() / len(y_train)\n",
    "\n",
    "#проверим на одном семпле\n",
    "test_sample = test_tensor[0]\n",
    "\n",
    "\n",
    "# посчитает значение за спам (формула)\n",
    "np.log(p_spam) + (np.log((test_sample*p_w_spam)+0.00000001)).sum()\n",
    "#посчитаем значение против спама\n",
    "np.log(p_not_spam) + (np.log((test_sample*p_w_not_spam)+0.00000001)).sum()\n",
    "# смотрим какое из этих значений больше и принмаем решение спам или не спам\n",
    "\n",
    "\n",
    "###################\n",
    "#посчитаем для всей тестовой выборки\n",
    "#размер тестовой выборки\n",
    "test_tensor.shape\n",
    "\n",
    "#посчитаем предсказания как сравнение величинв за спам и против спама\n",
    "y_pred = (np.log(p_spam) + (np.log((test_tensor*p_w_spam)+0.00000001)).sum(axis=1)) >= \\\n",
    " np.log(p_not_spam) + (np.log((test_tensor*p_w_not_spam)+0.00000001)).sum(axis=1)\n",
    "\n",
    "# предсказанные значения\n",
    "y_pred.astype(int)\n",
    "\n",
    "#истиные значение\n",
    "y_test.to_numpy()\n",
    "\n",
    "#сравним реальные метки с предсказанными\n",
    "test = (y_pred.astype(int) == y_test.to_numpy())\n",
    "\n",
    "#посчитаем точность модели как отношение количество совпадений к размеру выборки  \n",
    "test.sum().item()/test.shape[0]\n",
    "\n",
    "print(classification_report(y_test.to_numpy(), y_pred.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5703c92-e2c5-4f51-a77f-5b677593128c",
   "metadata": {},
   "source": [
    "## 23.6. Функцию as_matrix принимает текстовые последовательности, а возвращает тензор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d46575-0664-4a72-8aea-ac33fd2f1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map() -  когда нужно применить функцию преобразования к каждому элементу в коллекции или в массиве и преобразовать их в новый массив.\n",
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "# фун перевожит набор текстовых последовательностей в матрицу (тензор)\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):  # enumerate() – сразу индекс элемента и его значение.\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "### example of working\n",
    "## input sequences:\n",
    "# engineering systems analyst \n",
    "# hr assistant\n",
    "# senior ec & i engineer\n",
    "\n",
    "\n",
    "## output matrix:\n",
    "# [[10807 30161  2166     1     1]\n",
    "# [15020  2844     1     1     1]\n",
    "# [27645 10201    16 15215 10804]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
